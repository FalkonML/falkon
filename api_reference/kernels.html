

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>falkon.kernels &mdash; falkon 0.9.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=1bc1ca25"></script>
      <script src="../_static/doctools.js?v=888ff710"></script>
      <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="falkon.options" href="options.html" />
    <link rel="prev" title="falkon.models" href="models.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            falkon
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../install.html">Install</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../install.html#supported-platforms">Supported Platforms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#prerequisites">Prerequisites</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../install.html#pytorch-and-cuda">PyTorch and CUDA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../install.html#intel-mkl">Intel MKL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#installing">Installing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#testing-the-installation">Testing the installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#development">Development</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../get_started.html">Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../get_started.html#passing-options">Passing Options</a></li>
<li class="toctree-l2"><a class="reference internal" href="../get_started.html#more-examples">More Examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/examples.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/falkon_regression_tutorial.html">Falkon Regression Tutorial</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/falkon_regression_tutorial.html#Introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/falkon_regression_tutorial.html#Load-the-data">Load the data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/falkon_regression_tutorial.html#Pre-process-the-data">Pre-process the data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/falkon_regression_tutorial.html#Create-the-Falkon-model">Create the Falkon model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/falkon_regression_tutorial.html#Training-the-model">Training the model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/falkon_regression_tutorial.html#Evaluating-model-performance">Evaluating model performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/logistic_falkon.html">Introducing Logistic Falkon</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/logistic_falkon.html#Introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/logistic_falkon.html#Load-the-data">Load the data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/logistic_falkon.html#Split-into-training-and-test-sets">Split into training and test sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/logistic_falkon.html#Data-Preprocessing">Data Preprocessing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/logistic_falkon.html#Define-the-Falkon-model">Define the Falkon model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/logistic_falkon.html#Define-Logistic-Falkon-model">Define Logistic Falkon model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/logistic_falkon.html#Train-both-models">Train both models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/logistic_falkon.html#Testing">Testing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/logistic_falkon.html#Plot-predictions">Plot predictions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/falkon_cv.html">Hyperparameter Tuning with Falkon</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/falkon_cv.html#Introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/falkon_cv.html#Load-the-data">Load the data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/falkon_cv.html#Split-into-training-and-test-sets">Split into training and test sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/falkon_cv.html#Data-Preprocessing">Data Preprocessing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/falkon_cv.html#Search-for-the-optimal-parameters">Search for the optimal parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/falkon_cv.html#Evaluating-the-model">Evaluating the model</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/falkon_cv.html#Plot-grid-search-results">Plot grid-search results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/custom_kernels.html">Implementing A Custom Kernel</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/custom_kernels.html#Setup-a-simple-problem-for-testing">Setup a simple problem for testing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/custom_kernels.html#Basic-Kernel-Implementation">Basic Kernel Implementation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/custom_kernels.html#Test-the-basic-kernel">Test the basic kernel</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/custom_kernels.html#Differentiable-Kernel">Differentiable Kernel</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/custom_kernels.html#Test-the-differentiable-kernel">Test the differentiable kernel</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/custom_kernels.html#Adding-KeOps-Support">Adding KeOps Support</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/custom_kernels.html#Test-the-KeOps-kernel">Test the KeOps kernel</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/custom_kernels.html#Supporting-Sparse-Data">Supporting Sparse Data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/custom_kernels.html#Testing-sparse-support">Testing sparse support</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/hyperopt.html">Automatic Hyperparameter Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/hyperopt.html#Load-the-data">Load the data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/hyperopt.html#Split-into-training-and-test-sets">Split into training and test sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/hyperopt.html#Data-Preprocessing">Data Preprocessing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/hyperopt.html#Hyperparameter-Optimization">Hyperparameter Optimization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/falkon_mnist.html">MNIST Classification with Falkon</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/falkon_mnist.html#Download-the-MNIST-dataset-&amp;-load-it-in-memory">Download the MNIST dataset &amp; load it in memory</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/falkon_mnist.html#Data-Preprocessing">Data Preprocessing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/falkon_mnist.html#Run-Falkon">Run Falkon</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">API Reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="models.html">falkon.models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="models.html#falkon">Falkon</a><ul>
<li class="toctree-l4"><a class="reference internal" href="models.html#falkon.models.Falkon"><code class="docutils literal notranslate"><span class="pre">Falkon</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="models.html#logisticfalkon">LogisticFalkon</a><ul>
<li class="toctree-l4"><a class="reference internal" href="models.html#falkon.models.LogisticFalkon"><code class="docutils literal notranslate"><span class="pre">LogisticFalkon</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="models.html#incorefalkon">InCoreFalkon</a><ul>
<li class="toctree-l4"><a class="reference internal" href="models.html#falkon.models.InCoreFalkon"><code class="docutils literal notranslate"><span class="pre">InCoreFalkon</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">falkon.kernels</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#kernel">Kernel</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#falkon.kernels.kernel.Kernel"><code class="docutils literal notranslate"><span class="pre">Kernel</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#diffkernel">DiffKernel</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#falkon.kernels.diff_kernel.DiffKernel"><code class="docutils literal notranslate"><span class="pre">DiffKernel</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#keopskernelmixin">KeopsKernelMixin</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#falkon.kernels.keops_helpers.KeopsKernelMixin"><code class="docutils literal notranslate"><span class="pre">KeopsKernelMixin</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#radial-kernels">Radial kernels</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#gaussian-kernel">Gaussian kernel</a></li>
<li class="toctree-l4"><a class="reference internal" href="#laplacian-kernel">Laplacian kernel</a></li>
<li class="toctree-l4"><a class="reference internal" href="#matern-kernel">Matern kernel</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#dot-product-kernels">Dot-Product kernels</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#polynomial-kernel">Polynomial kernel</a></li>
<li class="toctree-l4"><a class="reference internal" href="#linear-kernel">Linear kernel</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sigmoid-kernel">Sigmoid kernel</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="options.html">falkon.options</a><ul>
<li class="toctree-l3"><a class="reference internal" href="options.html#falkonoptions">FalkonOptions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="options.html#falkon.options.FalkonOptions"><code class="docutils literal notranslate"><span class="pre">FalkonOptions</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="gsc_losses.html">falkon.gsc_losses</a><ul>
<li class="toctree-l3"><a class="reference internal" href="gsc_losses.html#loss">Loss</a><ul>
<li class="toctree-l4"><a class="reference internal" href="gsc_losses.html#falkon.gsc_losses.Loss"><code class="docutils literal notranslate"><span class="pre">Loss</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="gsc_losses.html#logistic-loss">Logistic loss</a><ul>
<li class="toctree-l4"><a class="reference internal" href="gsc_losses.html#falkon.gsc_losses.LogisticLoss"><code class="docutils literal notranslate"><span class="pre">LogisticLoss</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="gsc_losses.html#weighted-binary-cross-entropy-loss">Weighted binary cross entropy loss</a><ul>
<li class="toctree-l4"><a class="reference internal" href="gsc_losses.html#falkon.gsc_losses.WeightedCrossEntropyLoss"><code class="docutils literal notranslate"><span class="pre">WeightedCrossEntropyLoss</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="preconditioner.html">falkon.preconditioner</a><ul>
<li class="toctree-l3"><a class="reference internal" href="preconditioner.html#preconditioner">Preconditioner</a><ul>
<li class="toctree-l4"><a class="reference internal" href="preconditioner.html#falkon.preconditioner.preconditioner.Preconditioner"><code class="docutils literal notranslate"><span class="pre">Preconditioner</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="preconditioner.html#cholesky-preconditioners">Cholesky preconditioners</a><ul>
<li class="toctree-l4"><a class="reference internal" href="preconditioner.html#falkonpreconditioner">FalkonPreconditioner</a></li>
<li class="toctree-l4"><a class="reference internal" href="preconditioner.html#logisticpreconditioner">LogisticPreconditioner</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="optimization.html">falkon.optim</a><ul>
<li class="toctree-l3"><a class="reference internal" href="optimization.html#optimizer">Optimizer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="optimization.html#falkon.optim.Optimizer"><code class="docutils literal notranslate"><span class="pre">Optimizer</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="optimization.html#conjugate-gradient-methods">Conjugate gradient methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="optimization.html#conjugategradient">ConjugateGradient</a></li>
<li class="toctree-l4"><a class="reference internal" href="optimization.html#falkonconjugategradient">FalkonConjugateGradient</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="outofcore.html">falkon.ooc_ops</a><ul>
<li class="toctree-l3"><a class="reference internal" href="outofcore.html#gpu-cholesky">gpu_cholesky</a><ul>
<li class="toctree-l4"><a class="reference internal" href="outofcore.html#falkon.ooc_ops.gpu_cholesky"><code class="docutils literal notranslate"><span class="pre">gpu_cholesky()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="outofcore.html#gpu-lauum">gpu_lauum</a><ul>
<li class="toctree-l4"><a class="reference internal" href="outofcore.html#falkon.ooc_ops.gpu_lauum"><code class="docutils literal notranslate"><span class="pre">gpu_lauum()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="mmv_ops.html">falkon.mmv_ops</a><ul>
<li class="toctree-l3"><a class="reference internal" href="mmv_ops.html#run-keops-mmv">run_keops_mmv</a><ul>
<li class="toctree-l4"><a class="reference internal" href="mmv_ops.html#falkon.mmv_ops.keops.run_keops_mmv"><code class="docutils literal notranslate"><span class="pre">run_keops_mmv()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="mmv_ops.html#fmm">fmm</a><ul>
<li class="toctree-l4"><a class="reference internal" href="mmv_ops.html#falkon.mmv_ops.fmm.fmm"><code class="docutils literal notranslate"><span class="pre">fmm()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="mmv_ops.html#fmmv">fmmv</a><ul>
<li class="toctree-l4"><a class="reference internal" href="mmv_ops.html#falkon.mmv_ops.fmmv.fmmv"><code class="docutils literal notranslate"><span class="pre">fmmv()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="mmv_ops.html#fdmmv">fdmmv</a><ul>
<li class="toctree-l4"><a class="reference internal" href="mmv_ops.html#falkon.mmv_ops.fmmv.fdmmv"><code class="docutils literal notranslate"><span class="pre">fdmmv()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="mmv_ops.html#incore-fmmv">incore_fmmv</a><ul>
<li class="toctree-l4"><a class="reference internal" href="mmv_ops.html#falkon.mmv_ops.fmmv_incore.incore_fmmv"><code class="docutils literal notranslate"><span class="pre">incore_fmmv()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="mmv_ops.html#incore-fdmmv">incore_fdmmv</a><ul>
<li class="toctree-l4"><a class="reference internal" href="mmv_ops.html#falkon.mmv_ops.fmmv_incore.incore_fdmmv"><code class="docutils literal notranslate"><span class="pre">incore_fdmmv()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="mmv_ops.html#low-level-functions">Low-level functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="mmv_ops.html#falkon.mmv_ops.fmm.sparse_mm_run_thread"><code class="docutils literal notranslate"><span class="pre">sparse_mm_run_thread()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="mmv_ops.html#falkon.mmv_ops.fmmv.sparse_mmv_run_thread"><code class="docutils literal notranslate"><span class="pre">sparse_mmv_run_thread()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="sparse.html">falkon.sparse</a><ul>
<li class="toctree-l3"><a class="reference internal" href="sparse.html#sparsetensor">SparseTensor</a><ul>
<li class="toctree-l4"><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor"><code class="docutils literal notranslate"><span class="pre">SparseTensor</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseType"><code class="docutils literal notranslate"><span class="pre">SparseType</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="sparse.html#sparse-operations">Sparse operations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="sparse.html#falkon.sparse.sparse_matmul"><code class="docutils literal notranslate"><span class="pre">sparse_matmul()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="sparse.html#falkon.sparse.sparse_square_norm"><code class="docutils literal notranslate"><span class="pre">sparse_square_norm()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="sparse.html#falkon.sparse.sparse_norm"><code class="docutils literal notranslate"><span class="pre">sparse_norm()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="center_selector.html">falkon.center_selection</a><ul>
<li class="toctree-l3"><a class="reference internal" href="center_selector.html#centerselector">CenterSelector</a><ul>
<li class="toctree-l4"><a class="reference internal" href="center_selector.html#falkon.center_selection.CenterSelector"><code class="docutils literal notranslate"><span class="pre">CenterSelector</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="center_selector.html#uniformselector">UniformSelector</a><ul>
<li class="toctree-l4"><a class="reference internal" href="center_selector.html#falkon.center_selection.UniformSelector"><code class="docutils literal notranslate"><span class="pre">UniformSelector</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="center_selector.html#fixedselector">FixedSelector</a><ul>
<li class="toctree-l4"><a class="reference internal" href="center_selector.html#falkon.center_selection.FixedSelector"><code class="docutils literal notranslate"><span class="pre">FixedSelector</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="hopt.html">falkon.hopt</a><ul>
<li class="toctree-l3"><a class="reference internal" href="hopt.html#objectives">Objectives</a><ul>
<li class="toctree-l4"><a class="reference internal" href="hopt.html#falkon.hopt.objectives.objectives.HyperoptObjective"><code class="docutils literal notranslate"><span class="pre">HyperoptObjective</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="hopt.html#nystrom-complexity-regularization">Nystrom Complexity Regularization</a></li>
<li class="toctree-l4"><a class="reference internal" href="hopt.html#stochastic-nystrom-computational-regularization">Stochastic Nystrom Computational Regularization</a></li>
<li class="toctree-l4"><a class="reference internal" href="hopt.html#complexity-regularization">Complexity Regularization</a></li>
<li class="toctree-l4"><a class="reference internal" href="hopt.html#generalized-cross-validation">Generalized Cross Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="hopt.html#hold-out-cross-validation">Hold Out Cross Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="hopt.html#leave-one-out-cross-validation">Leave One Out Cross Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="hopt.html#sgpr">SGPR</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">falkon</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">API Reference</a></li>
      <li class="breadcrumb-item active">falkon.kernels</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/api_reference/kernels.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-falkon.kernels">
<span id="falkon-kernels"></span><h1>falkon.kernels<a class="headerlink" href="#module-falkon.kernels" title="Permalink to this heading"></a></h1>
<section id="kernel">
<h2>Kernel<a class="headerlink" href="#kernel" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="falkon.kernels.kernel.Kernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">falkon.kernels.kernel.</span></span><span class="sig-name descname"><span class="pre">Kernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.kernel.Kernel" title="Permalink to this definition"></a></dt>
<dd><p>Abstract kernel class. Kernels should inherit from this class, overriding appropriate methods.</p>
<p>To extend Falkon with new kernels, you should read the documentation of this class
carefully, and take a look at the existing implementation of <a class="reference internal" href="#falkon.kernels.GaussianKernel" title="falkon.kernels.GaussianKernel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GaussianKernel</span></code></a>
or <a class="reference internal" href="#falkon.kernels.LinearKernel" title="falkon.kernels.LinearKernel"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearKernel</span></code></a>. A walk-through for implementing a custom kernel
is available as a <a class="reference internal" href="../examples/custom_kernels.html"><span class="doc">notebook</span></a>.</p>
<p>There are several abstract methods which should be implemented, depending on the kind of operations
which are supported by the implementing kernel.</p>
<p>The <a class="reference internal" href="#falkon.kernels.kernel.Kernel.compute" title="falkon.kernels.kernel.Kernel.compute"><code class="xref py py-meth docutils literal notranslate"><span class="pre">compute()</span></code></a> method should compute the kernel matrix, without concerns for differentiability,
while the <a class="reference internal" href="#falkon.kernels.kernel.Kernel.compute_sparse" title="falkon.kernels.kernel.Kernel.compute_sparse"><code class="xref py py-meth docutils literal notranslate"><span class="pre">compute_sparse()</span></code></a> method is used to compute the kernel for sparse input matrices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – A short name for the kernel (e.g. “Gaussian”)</p></li>
<li><p><strong>opt</strong> – Base set of options to be used for operations involving this kernel.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.kernel.Kernel.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diag</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#falkon.kernels.kernel.Kernel.__call__" title="Permalink to this definition"></a></dt>
<dd><p>Compute the kernel matrix between <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <code class="docutils literal notranslate"><span class="pre">X2</span> <span class="pre">==</span> <span class="pre">X1</span></code> to compute a symmetric kernel.</p></li>
<li><p><strong>diag</strong> (<em>bool</em>) – Whether to compute just the diagonal of the kernel matrix, or the whole matrix.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (N x M) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options to be used for computing the operation. Useful are the memory size options
and CUDA options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The kernel between <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.kernel.Kernel._decide_dmmv_impl">
<span class="sig-name descname"><span class="pre">_decide_dmmv_impl</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">w</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.kernel.Kernel._decide_dmmv_impl" title="Permalink to this definition"></a></dt>
<dd><p>Choose which <cite>dmmv</cite> function to use for this data.</p>
<p>Note that <cite>dmmv</cite> functions compute double kernel-vector products (see <a class="reference internal" href="#falkon.kernels.kernel.Kernel.dmmv" title="falkon.kernels.kernel.Kernel.dmmv"><code class="xref py py-meth docutils literal notranslate"><span class="pre">dmmv()</span></code></a> for
an explanation of what they are).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – First data matrix, of shape (N x D)</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – Second data matrix, of shape (M x D)</p></li>
<li><p><strong>v</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Vector for the matrix-vector multiplication (M x T)</p></li>
<li><p><strong>w</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Vector for the matrix-vector multiplicatoin (N x T)</p></li>
<li><p><strong>opt</strong> (<a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a>) – Falkon options. Options may be specified to force GPU or CPU usage.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><em>dmmv_fn</em> – A function which allows to perform the <cite>mmv</cite> operation.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This function decides based on the inputs: if the inputs are sparse, it will choose
the sparse implementations; if CUDA is detected, it will choose the CUDA implementation;
otherwise it will simply choose the basic CPU implementation.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.kernel.Kernel._decide_mm_impl">
<span class="sig-name descname"><span class="pre">_decide_mm_impl</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diag</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.kernel.Kernel._decide_mm_impl" title="Permalink to this definition"></a></dt>
<dd><p>Choose which <cite>mm</cite> function to use for this data.</p>
<p>Note that <cite>mm</cite> functions compute the kernel itself so <strong>KeOps may not be used</strong>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – First data matrix, of shape (N x D)</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – Second data matrix, of shape (M x D)</p></li>
<li><p><strong>diag</strong> (<em>bool</em>) – Whether to compute just the diagonal of the kernel matrix, or the whole matrix.</p></li>
<li><p><strong>opt</strong> (<a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a>) – Falkon options. Options may be specified to force GPU or CPU usage.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><em>mm_fn</em> – A function which allows to perform the <cite>mm</cite> operation.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This function decides based on the inputs: if the inputs are sparse, it will choose
the sparse implementations; if CUDA is detected, it will choose the CUDA implementation;
otherwise it will simply choose the basic CPU implementation.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.kernel.Kernel._decide_mmv_impl">
<span class="sig-name descname"><span class="pre">_decide_mmv_impl</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.kernel.Kernel._decide_mmv_impl" title="Permalink to this definition"></a></dt>
<dd><p>Choose which <cite>mmv</cite> function to use for this data.</p>
<p>Note that <cite>mmv</cite> functions compute the kernel-vector product</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – First data matrix, of shape (N x D)</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – Second data matrix, of shape (M x D)</p></li>
<li><p><strong>v</strong> (<em>torch.Tensor</em>) – Vector for the matrix-vector multiplication (M x T)</p></li>
<li><p><strong>opt</strong> (<a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a>) – Falkon options. Options may be specified to force GPU or CPU usage.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><em>mmv_fn</em> – A function which allows to perform the <cite>mmv</cite> operation.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This function decides based on the inputs: if the inputs are sparse, it will choose
the sparse implementations; if CUDA is detected, it will choose the CUDA implementation;
otherwise it will simply choose the basic CPU implementation.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.kernel.Kernel.compute">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">compute</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diag</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#falkon.kernels.kernel.Kernel.compute" title="Permalink to this definition"></a></dt>
<dd><p>Compute the kernel matrix of <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code> - without regards for differentiability.</p>
<p>The kernel matrix should be stored in <code class="docutils literal notranslate"><span class="pre">out</span></code> to ensure the correctness of allocatable
memory computations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The left matrix for computing the kernel</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The right matrix for computing the kernel</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em>) – The output matrix into which implementing classes should store the kernel.</p></li>
<li><p><strong>diag</strong> (<em>bool</em>) – If true, <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code> have the same shape, and only the diagonal of <code class="docutils literal notranslate"><span class="pre">k(X1,</span> <span class="pre">X2)</span></code>
is to be computed and stored in <code class="docutils literal notranslate"><span class="pre">out</span></code>. Otherwise compute the full kernel matrix.</p></li>
<li><p><strong>kwargs</strong> – Additional keyword arguments which may be used in computing the kernel values</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The kernel matrix. Should use the same underlying storage as the parameter <code class="docutils literal notranslate"><span class="pre">out</span></code>.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Supporting the <cite>diag</cite> argument is <strong>optional</strong>. It’s only used with in the hyper-parameter
optimization module, so if you’re not using that you don’t need to implement
this function for <cite>diag=True</cite> (it will always be False).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.kernel.Kernel.compute_sparse">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">compute_sparse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diag</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#falkon.kernels.kernel.Kernel.compute_sparse" title="Permalink to this definition"></a></dt>
<dd><p>Compute the kernel matrix of <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code> which are two sparse matrices, storing the output
in the dense matrix <code class="docutils literal notranslate"><span class="pre">out</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><em>SparseTensor</em></a>) – The left matrix for computing the kernel</p></li>
<li><p><strong>X2</strong> (<a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><em>SparseTensor</em></a>) – The right matrix for computing the kernel</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em>) – The output matrix into which implementing classes should store the kernel.</p></li>
<li><p><strong>diag</strong> (<em>bool</em>) – If true, <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code> have the same shape, and only the diagonal of <code class="docutils literal notranslate"><span class="pre">k(X1,</span> <span class="pre">X2)</span></code>
is to be computed and stored in <code class="docutils literal notranslate"><span class="pre">out</span></code>.</p></li>
<li><p><strong>kwargs</strong> – <p>Additional keyword arguments which some sparse implementations might require. Currently
the keyword arguments passed by the <a class="reference internal" href="mmv_ops.html#falkon.mmv_ops.fmmv.sparse_mmv_run_thread" title="falkon.mmv_ops.fmmv.sparse_mmv_run_thread"><code class="xref py py-func docutils literal notranslate"><span class="pre">falkon.mmv_ops.fmmv.sparse_mmv_run_thread()</span></code></a>
and <a class="reference internal" href="mmv_ops.html#falkon.mmv_ops.fmm.sparse_mm_run_thread" title="falkon.mmv_ops.fmm.sparse_mm_run_thread"><code class="xref py py-func docutils literal notranslate"><span class="pre">falkon.mmv_ops.fmm.sparse_mm_run_thread()</span></code></a> functions are:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">X1_csr</span></code> : the <code class="docutils literal notranslate"><span class="pre">X1</span></code> matrix in CSR format</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">X2_csr</span></code> : the <code class="docutils literal notranslate"><span class="pre">X2</span></code> matrix in CSR format</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The kernel matrix. Should use the same underlying storage as the parameter <code class="docutils literal notranslate"><span class="pre">out</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.kernel.Kernel.dmmv">
<span class="sig-name descname"><span class="pre">dmmv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">w</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.kernel.Kernel.dmmv" title="Permalink to this definition"></a></dt>
<dd><p>Compute double matrix-vector multiplications where the matrix is the current kernel.</p>
<p>The general form of <cite>dmmv</cite> operations is: <cite>Kernel(X2, X1) &#64; (Kernel(X1, X2) &#64; v + w)</cite>
where if <cite>v</cite> is None, then we simply have <cite>Kernel(X2, X1) &#64; w</cite> and if <cite>w</cite> is None
we remove the additive factor.
<strong>At least one of `w` and `v` must be provided</strong>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <cite>X2 == X1</cite> to compute a symmetric kernel.</p></li>
<li><p><strong>v</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – A vector to compute the matrix-vector product. This may also be a matrix of shape
(M x T), but if <cite>T</cite> is very large the operations will be much slower.</p></li>
<li><p><strong>w</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – A vector to compute matrix-vector products. This may also be a matrix of shape
(N x T) but if <cite>T</cite> is very large the operations will be much slower.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (M x T) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options to be used for computing the operation. Useful are the memory size options
and CUDA options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">X1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">X1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">X2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">X2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The (M x T) output.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">falkon</span><span class="o">,</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">falkon</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">GaussianKernel</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># You can substitute the Gaussian kernel by any other.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># N is 100, D is 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># M is 150</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">dmmv</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([150, 1])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.kernel.Kernel.extra_mem">
<span class="sig-name descname"><span class="pre">extra_mem</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">is_differentiable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_sparse</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">density1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">density2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#falkon.kernels.kernel.Kernel.extra_mem" title="Permalink to this definition"></a></dt>
<dd><p>Compute the amount of extra memory which will be needed when computing this kernel.</p>
<p>Often kernel computation needs some extra memory allocations. To avoid using too large
block-sizes which may lead to OOM errors, you should declare any such extra allocations
for your kernel here.</p>
<p>Indicate extra allocations as coefficients on the required dimensions. For example,
if computing a kernel needs to re-allocate the data-matrix (which is of size n * d),
the return dictionary will be: <cite>{‘nd’: 1}</cite>. Other possible coefficients are on <cite>d</cite>, <cite>n</cite>, <cite>m</cite>
which are respectively the data-dimension, the number of data-points in the first data
matrix and the number of data-points in the second matrix. Pairwise combinations of the
three dimensions are possible (i.e. <cite>nd</cite>, <cite>nm</cite>, <cite>md</cite>), and a special key ‘0’ can be
used to specify a base memory needed independently of data dimensions.
Make sure to specify the dictionary keys as is written here since they will not be
recognized otherwise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>is_differentiable</strong> (<em>bool</em>) – </p></li>
<li><p><strong>is_sparse</strong> (<em>bool</em>) – </p></li>
<li><p><strong>dtype</strong> (<em>torch.dtype</em><em> or </em><em>np.dtype</em>) – </p></li>
<li><p><strong>density1</strong> (<em>float</em><em> or </em><em>None</em>) – </p></li>
<li><p><strong>density2</strong> (<em>float</em><em> or </em><em>None</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>extra_allocs</strong> (<em>dictionary</em>) – A dictionary from strings indicating on which dimensions the extra-allocation is
needed (allowed strings: <cite>‘n’, ‘m’, ‘d’, ‘nm’, ‘nd’, ‘md’, ‘0’</cite>) to floating-point
numbers indicating how many extra-allocations are needed.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.kernel.Kernel.mmv">
<span class="sig-name descname"><span class="pre">mmv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.kernel.Kernel.mmv" title="Permalink to this definition"></a></dt>
<dd><p>Compute matrix-vector multiplications where the matrix is the current kernel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <cite>X2 == X1</cite> to compute a symmetric kernel.</p></li>
<li><p><strong>v</strong> (<em>torch.Tensor</em>) – A vector to compute the matrix-vector product. This may also be a matrix of shape
(M x T), but if <cite>T</cite> is very large the operations will be much slower.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (N x T) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options to be used for computing the operation. Useful are the memory size options
and CUDA options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The (N x T) output.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">falkon</span><span class="o">,</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">falkon</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">GaussianKernel</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># You can substitute the Gaussian kernel by any other.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">mmv</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([100, 1])</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="diffkernel">
<h2>DiffKernel<a class="headerlink" href="#diffkernel" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="falkon.kernels.diff_kernel.DiffKernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">falkon.kernels.diff_kernel.</span></span><span class="sig-name descname"><span class="pre">DiffKernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">options</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">core_fn</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kernel_params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.diff_kernel.DiffKernel" title="Permalink to this definition"></a></dt>
<dd><p>Abstract class for differentiable kernels.</p>
<p>This class should be extended instead of <a class="reference internal" href="#falkon.kernels.kernel.Kernel" title="falkon.kernels.kernel.Kernel"><code class="xref py py-class docutils literal notranslate"><span class="pre">Kernel</span></code></a> whenever designing
a custom kernel to be used with automatic hyperparameter optimization (see the <a class="reference internal" href="hopt.html#module-falkon.hopt" title="falkon.hopt"><code class="xref py py-mod docutils literal notranslate"><span class="pre">hopt</span></code></a>
module).</p>
<p>Subclasses should implement the <a class="reference internal" href="#falkon.kernels.diff_kernel.DiffKernel.detach" title="falkon.kernels.diff_kernel.DiffKernel.detach"><code class="xref py py-meth docutils literal notranslate"><span class="pre">detach()</span></code></a> method to return a new instance of the kernel,
with its parameters detached from the computation graph.</p>
<p>The <a class="reference internal" href="#falkon.kernels.diff_kernel.DiffKernel.compute_diff" title="falkon.kernels.diff_kernel.DiffKernel.compute_diff"><code class="xref py py-meth docutils literal notranslate"><span class="pre">compute_diff()</span></code></a> method should be overridden, unless the <code class="docutils literal notranslate"><span class="pre">core_fn</span></code> parameter is
passed to the constructor.</p>
<p>Hyperparameters to the concrete kernel instance (for example the length-scale of the Gaussian
kernel) should be passed to the constructor of this class, in order to be registered
as parameters of the computation graph. Even non-differentiable parameters should be provided
as keywords (also non tensor arguments).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – A short name for the kernel (e.g. “Gaussian”)</p></li>
<li><p><strong>options</strong> – Base set of options to be used for operations involving this kernel.</p></li>
<li><p><strong>core_fn</strong> – Optional function which can be used to compute a kernel matrix.
The signature of the function should be:
<code class="docutils literal notranslate"><span class="pre">core_fn(X1,</span> <span class="pre">X2,</span> <span class="pre">out,</span> <span class="pre">diag,</span> <span class="pre">**kernel_parameters)`</span></code>
where <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code> are the input matrices, <code class="docutils literal notranslate"><span class="pre">out</span></code> is the output matrix (it will
be <code class="docutils literal notranslate"><span class="pre">None</span></code> when called from <a class="reference internal" href="#falkon.kernels.diff_kernel.DiffKernel.compute_diff" title="falkon.kernels.diff_kernel.DiffKernel.compute_diff"><code class="xref py py-meth docutils literal notranslate"><span class="pre">compute_diff()</span></code></a>), <code class="docutils literal notranslate"><span class="pre">diag</span></code> is a flag indicating
that only the diagonal of the kernel matrix is to be computed, and <code class="docutils literal notranslate"><span class="pre">**kernel_parameters</span></code>
includes all additional parameters belonging to the kernel (which are passed to the
constructor of <a class="reference internal" href="#falkon.kernels.diff_kernel.DiffKernel" title="falkon.kernels.diff_kernel.DiffKernel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DiffKernel</span></code></a>).</p></li>
<li><p><strong>kernel_params</strong> – All parameters (differentiable and non-differentiable) to be used for this kernel.
The values given are used to initialize the actual parameters - which will be copied in
the constructor.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.diff_kernel.DiffKernel.compute_diff">
<span class="sig-name descname"><span class="pre">compute_diff</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diag</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.diff_kernel.DiffKernel.compute_diff" title="Permalink to this definition"></a></dt>
<dd><p>Compute the kernel matrix of <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code>. The output should be differentiable with
respect to <cite>X1</cite>, <cite>X2</cite>, and all kernel parameters returned by the <a class="reference internal" href="#falkon.kernels.diff_kernel.DiffKernel.diff_params" title="falkon.kernels.diff_kernel.DiffKernel.diff_params"><code class="xref py py-meth docutils literal notranslate"><span class="pre">diff_params()</span></code></a> method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The left matrix for computing the kernel</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The right matrix for computing the kernel</p></li>
<li><p><strong>diag</strong> (<em>bool</em>) – If true, <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code> have the same shape, and only the diagonal of <code class="docutils literal notranslate"><span class="pre">k(X1,</span> <span class="pre">X2)</span></code>
is to be computed and stored in <code class="docutils literal notranslate"><span class="pre">out</span></code>. Otherwise compute the full kernel matrix.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The constructed kernel matrix.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.diff_kernel.DiffKernel.detach">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">detach</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#falkon.kernels.kernel.Kernel" title="falkon.kernels.kernel.Kernel"><span class="pre">Kernel</span></a></span></span><a class="headerlink" href="#falkon.kernels.diff_kernel.DiffKernel.detach" title="Permalink to this definition"></a></dt>
<dd><p>Detaches all differentiable parameters of the kernel from the computation graph.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>k</em> – A new instance of the kernel sharing the same parameters, but detached from the
computation graph.</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="falkon.kernels.diff_kernel.DiffKernel.diff_params">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">diff_params</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#falkon.kernels.diff_kernel.DiffKernel.diff_params" title="Permalink to this definition"></a></dt>
<dd><p>A dictionary mapping parameter names to their values for all <strong>differentiable</strong> parameters
of the kernel.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>params</em> – A dictionary mapping parameter names to their values</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="keopskernelmixin">
<h2>KeopsKernelMixin<a class="headerlink" href="#keopskernelmixin" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="falkon.kernels.keops_helpers.KeopsKernelMixin">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">falkon.kernels.keops_helpers.</span></span><span class="sig-name descname"><span class="pre">KeopsKernelMixin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.keops_helpers.KeopsKernelMixin" title="Permalink to this definition"></a></dt>
<dd><p>Abstract class for kernels which enables KeOps acceleration.</p>
<p>This class should be extended when KeOps-accelerated kernel-vector products are required.
Subclasses should implement the <a class="reference internal" href="#falkon.kernels.keops_helpers.KeopsKernelMixin.keops_mmv_impl" title="falkon.kernels.keops_helpers.KeopsKernelMixin.keops_mmv_impl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">keops_mmv_impl()</span></code></a> method by defining an appropriate
KeOps formula, and calling into KeOps for kernel vector product calculation (the helper method
<a class="reference internal" href="#falkon.kernels.keops_helpers.KeopsKernelMixin.keops_mmv" title="falkon.kernels.keops_helpers.KeopsKernelMixin.keops_mmv"><code class="xref py py-meth docutils literal notranslate"><span class="pre">keops_mmv()</span></code></a> can be used to call into KeOps).</p>
<p>For help in implementing a subclass, check the existing implementations
(e.g. <a class="reference internal" href="#falkon.kernels.GaussianKernel" title="falkon.kernels.GaussianKernel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GaussianKernel</span></code></a>), and the
<a class="reference internal" href="../examples/custom_kernels.html"><span class="doc">Custom Kernels</span></a> notebook.</p>
<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.keops_helpers.KeopsKernelMixin.keops_mmv">
<span class="sig-name descname"><span class="pre">keops_mmv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formula</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aliases</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other_vars</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.keops_helpers.KeopsKernelMixin.keops_mmv" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to call into KeOps for kernel-vector products</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <code class="docutils literal notranslate"><span class="pre">X2</span> <span class="pre">==</span> <span class="pre">X1</span></code> to compute a symmetric kernel.</p></li>
<li><p><strong>v</strong> (<em>torch.Tensor</em>) – A vector to compute the matrix-vector product. This may also be a matrix of shape
(M x T), but if <code class="docutils literal notranslate"><span class="pre">T</span></code> is very large the operations will be much slower.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (N x T) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>formula</strong> (<em>str</em>) – The KeOps formula</p></li>
<li><p><strong>aliases</strong> – Aliases referencing names in the formula with actual KeOps variables</p></li>
<li><p><strong>other_vars</strong> – Kernel parameters to be used in the formula, other than <code class="docutils literal notranslate"><span class="pre">X1</span></code>, <code class="docutils literal notranslate"><span class="pre">X2</span></code> and <code class="docutils literal notranslate"><span class="pre">v</span></code>.</p></li>
<li><p><strong>opt</strong> – Options to be used for computing the operation. Useful are the memory size options,
CUDA options and KeOps options.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><em>out</em> – The computed kernel matrix between <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code>, multiplied by vector <code class="docutils literal notranslate"><span class="pre">v</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.keops_helpers.KeopsKernelMixin.keops_mmv_impl">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">keops_mmv_impl</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.keops_helpers.KeopsKernelMixin.keops_mmv_impl" title="Permalink to this definition"></a></dt>
<dd><p>Implementation of the KeOps formula to compute a kernel-vector product.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <code class="docutils literal notranslate"><span class="pre">X2</span> <span class="pre">==</span> <span class="pre">X1</span></code> to compute a symmetric kernel.</p></li>
<li><p><strong>v</strong> (<em>torch.Tensor</em>) – A vector to compute the matrix-vector product. This may also be a matrix of shape
(M x T), but if <code class="docutils literal notranslate"><span class="pre">T</span></code> is very large the operations will be much slower.</p></li>
<li><p><strong>kernel</strong> (<a class="reference internal" href="#falkon.kernels.kernel.Kernel" title="falkon.kernels.kernel.Kernel"><em>falkon.kernels.kernel.Kernel</em></a>) – Instance of this class. This is equal to <code class="docutils literal notranslate"><span class="pre">self</span></code> and can be ignored.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (N x T) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a>) – Options to be used for computing the operation. Useful are the memory size options,
CUDA options and KeOps options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">X1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">X1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">X2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">X2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><em>out</em> – The computed kernel matrix between <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code>, multiplied by vector <code class="docutils literal notranslate"><span class="pre">v</span></code>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="radial-kernels">
<h2>Radial kernels<a class="headerlink" href="#radial-kernels" title="Permalink to this heading"></a></h2>
<section id="gaussian-kernel">
<h3>Gaussian kernel<a class="headerlink" href="#gaussian-kernel" title="Permalink to this heading"></a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="falkon.kernels.GaussianKernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">falkon.kernels.</span></span><span class="sig-name descname"><span class="pre">GaussianKernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.GaussianKernel" title="Permalink to this definition"></a></dt>
<dd><p>Class for computing the Gaussian kernel and related kernel-vector products</p>
<p>The Gaussian kernel is one of the most common and effective kernel embeddings
since it is infinite dimensional, and governed by a single parameter. The kernel length-scale
determines the width of the Gaussian distribution which is placed on top of each point.
A larger sigma corresponds to a wide Gaussian, so that the relative influence of far away
points will be high for computing the kernel at a given datum.
On the opposite side of the spectrum, a small sigma means that only nearby points will
influence the kernel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sigma</strong> – The length-scale of the kernel.
This can be a scalar, and then it corresponds to the standard deviation
of the Gaussian distribution from which the kernel is derived.
If <cite>sigma</cite> is a vector of size <cite>d</cite> (where <cite>d</cite> is the dimensionality of the data), it is
interpreted as the diagonal standard deviation of the Gaussian distribution.
It can also be a matrix of  size <cite>d*d</cite> where <cite>d</cite>, in which case sigma will be the precision
matrix (inverse covariance).</p></li>
<li><p><strong>opt</strong> – Additional options to be forwarded to the matrix-vector multiplication
routines.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Creating a Gaussian kernel with a single length-scale. Operations on this kernel will not
use KeOps.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">K</span> <span class="o">=</span> <span class="n">GaussianKernel</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">opt</span><span class="o">=</span><span class="n">FalkonOptions</span><span class="p">(</span><span class="n">keops_active</span><span class="o">=</span><span class="s2">&quot;no&quot;</span><span class="p">))</span>
</pre></div>
</div>
<p>Creating a Gaussian kernel with a different length-scale per dimension</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">K</span> <span class="o">=</span> <span class="n">GaussianKernel</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">]))</span>
</pre></div>
</div>
<p>Creating a Gaussian kernel object with full covariance matrix (randomly chosen)</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sym_mat</span> <span class="o">=</span> <span class="n">mat</span> <span class="o">@</span> <span class="n">mat</span><span class="o">.</span><span class="n">T</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">K</span> <span class="o">=</span> <span class="n">GaussianKernel</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="n">sym_mat</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">K</span>
<span class="go">GaussianKernel(sigma=tensor([[ 2.0909,  0.0253, -0.2490],</span>
<span class="go">        [ 0.0253,  0.3399, -0.5158],</span>
<span class="go">        [-0.2490, -0.5158,  4.4922]], dtype=torch.float64))  #random</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>The Gaussian kernel with a single length-scale follows</p>
<div class="math notranslate nohighlight">
\[k(x, x') = \exp{-\dfrac{\lVert x - x' \rVert^2}{2\sigma^2}}\]</div>
<p>When the length-scales are specified as a matrix, the RBF kernel is determined by</p>
<div class="math notranslate nohighlight">
\[k(x, x') = \exp{-\dfrac{1}{2}x\Sigma x'}\]</div>
<p>In both cases, the actual computation follows a different path, working on the expanded
norm.</p>
<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.GaussianKernel.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diag</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#falkon.kernels.GaussianKernel.__call__" title="Permalink to this definition"></a></dt>
<dd><p>Compute the kernel matrix between <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <code class="docutils literal notranslate"><span class="pre">X2</span> <span class="pre">==</span> <span class="pre">X1</span></code> to compute a symmetric kernel.</p></li>
<li><p><strong>diag</strong> (<em>bool</em>) – Whether to compute just the diagonal of the kernel matrix, or the whole matrix.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (N x M) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options to be used for computing the operation. Useful are the memory size options
and CUDA options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The kernel between <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.GaussianKernel.dmmv">
<span class="sig-name descname"><span class="pre">dmmv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">w</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.GaussianKernel.dmmv" title="Permalink to this definition"></a></dt>
<dd><p>Compute double matrix-vector multiplications where the matrix is the current kernel.</p>
<p>The general form of <cite>dmmv</cite> operations is: <cite>Kernel(X2, X1) &#64; (Kernel(X1, X2) &#64; v + w)</cite>
where if <cite>v</cite> is None, then we simply have <cite>Kernel(X2, X1) &#64; w</cite> and if <cite>w</cite> is None
we remove the additive factor.
<strong>At least one of `w` and `v` must be provided</strong>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <cite>X2 == X1</cite> to compute a symmetric kernel.</p></li>
<li><p><strong>v</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – A vector to compute the matrix-vector product. This may also be a matrix of shape
(M x T), but if <cite>T</cite> is very large the operations will be much slower.</p></li>
<li><p><strong>w</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – A vector to compute matrix-vector products. This may also be a matrix of shape
(N x T) but if <cite>T</cite> is very large the operations will be much slower.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (M x T) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options to be used for computing the operation. Useful are the memory size options
and CUDA options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">X1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">X1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">X2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">X2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The (M x T) output.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">falkon</span><span class="o">,</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">falkon</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">GaussianKernel</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># You can substitute the Gaussian kernel by any other.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># N is 100, D is 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># M is 150</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">dmmv</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([150, 1])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.GaussianKernel.mmv">
<span class="sig-name descname"><span class="pre">mmv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.GaussianKernel.mmv" title="Permalink to this definition"></a></dt>
<dd><p>Compute matrix-vector multiplications where the matrix is the current kernel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <cite>X2 == X1</cite> to compute a symmetric kernel.</p></li>
<li><p><strong>v</strong> (<em>torch.Tensor</em>) – A vector to compute the matrix-vector product. This may also be a matrix of shape
(M x T), but if <cite>T</cite> is very large the operations will be much slower.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (N x T) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options to be used for computing the operation. Useful are the memory size options
and CUDA options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The (N x T) output.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">falkon</span><span class="o">,</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">falkon</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">GaussianKernel</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># You can substitute the Gaussian kernel by any other.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">mmv</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([100, 1])</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="laplacian-kernel">
<h3>Laplacian kernel<a class="headerlink" href="#laplacian-kernel" title="Permalink to this heading"></a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="falkon.kernels.LaplacianKernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">falkon.kernels.</span></span><span class="sig-name descname"><span class="pre">LaplacianKernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.LaplacianKernel" title="Permalink to this definition"></a></dt>
<dd><p>Class for computing the Laplacian kernel, and related kernel-vector products.</p>
<p>The Laplacian kernel is similar to the Gaussian kernel, but less sensitive to changes
in the parameter <cite>sigma</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>sigma</strong> – The length-scale of the Laplacian kernel</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The Laplacian kernel is determined by the following formula</p>
<div class="math notranslate nohighlight">
\[k(x, x') = \exp{-\frac{\lVert x - x' \rVert}{\sigma}}\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.LaplacianKernel.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diag</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#falkon.kernels.LaplacianKernel.__call__" title="Permalink to this definition"></a></dt>
<dd><p>Compute the kernel matrix between <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <code class="docutils literal notranslate"><span class="pre">X2</span> <span class="pre">==</span> <span class="pre">X1</span></code> to compute a symmetric kernel.</p></li>
<li><p><strong>diag</strong> (<em>bool</em>) – Whether to compute just the diagonal of the kernel matrix, or the whole matrix.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (N x M) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options to be used for computing the operation. Useful are the memory size options
and CUDA options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The kernel between <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.LaplacianKernel.dmmv">
<span class="sig-name descname"><span class="pre">dmmv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">w</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.LaplacianKernel.dmmv" title="Permalink to this definition"></a></dt>
<dd><p>Compute double matrix-vector multiplications where the matrix is the current kernel.</p>
<p>The general form of <cite>dmmv</cite> operations is: <cite>Kernel(X2, X1) &#64; (Kernel(X1, X2) &#64; v + w)</cite>
where if <cite>v</cite> is None, then we simply have <cite>Kernel(X2, X1) &#64; w</cite> and if <cite>w</cite> is None
we remove the additive factor.
<strong>At least one of `w` and `v` must be provided</strong>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <cite>X2 == X1</cite> to compute a symmetric kernel.</p></li>
<li><p><strong>v</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – A vector to compute the matrix-vector product. This may also be a matrix of shape
(M x T), but if <cite>T</cite> is very large the operations will be much slower.</p></li>
<li><p><strong>w</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – A vector to compute matrix-vector products. This may also be a matrix of shape
(N x T) but if <cite>T</cite> is very large the operations will be much slower.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (M x T) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options to be used for computing the operation. Useful are the memory size options
and CUDA options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">X1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">X1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">X2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">X2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The (M x T) output.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">falkon</span><span class="o">,</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">falkon</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">GaussianKernel</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># You can substitute the Gaussian kernel by any other.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># N is 100, D is 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># M is 150</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">dmmv</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([150, 1])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.LaplacianKernel.mmv">
<span class="sig-name descname"><span class="pre">mmv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.LaplacianKernel.mmv" title="Permalink to this definition"></a></dt>
<dd><p>Compute matrix-vector multiplications where the matrix is the current kernel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <cite>X2 == X1</cite> to compute a symmetric kernel.</p></li>
<li><p><strong>v</strong> (<em>torch.Tensor</em>) – A vector to compute the matrix-vector product. This may also be a matrix of shape
(M x T), but if <cite>T</cite> is very large the operations will be much slower.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (N x T) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options to be used for computing the operation. Useful are the memory size options
and CUDA options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The (N x T) output.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">falkon</span><span class="o">,</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">falkon</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">GaussianKernel</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># You can substitute the Gaussian kernel by any other.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">mmv</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([100, 1])</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="matern-kernel">
<h3>Matern kernel<a class="headerlink" href="#matern-kernel" title="Permalink to this heading"></a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="falkon.kernels.MaternKernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">falkon.kernels.</span></span><span class="sig-name descname"><span class="pre">MaternKernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nu</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.MaternKernel" title="Permalink to this definition"></a></dt>
<dd><p>Class for computing the Matern kernel, and related kernel-vector products.</p>
<p>The Matern kernels define a generic class of kernel functions which includes the
Laplacian and Gaussian kernels. The class is parametrized by ‘nu’. When <code class="docutils literal notranslate"><span class="pre">nu</span> <span class="pre">=</span> <span class="pre">0.5</span></code>
this kernel is equivalent to the Laplacian kernel, when <code class="docutils literal notranslate"><span class="pre">nu</span> <span class="pre">=</span> <span class="pre">float('inf')</span></code>, the
Matern kernel is equivalent to the Gaussian kernel.</p>
<p>This class implements the Matern kernel only for the values of nu which have a closed
form solution, which are 0.5, 1.5, 2.5, and infinity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sigma</strong> – The length-scale of the Matern kernel. The length-scale can be either a scalar
or a vector. Matrix-valued length-scales are not allowed for the Matern kernel.</p></li>
<li><p><strong>nu</strong> – The parameter of the Matern kernel. It should be one of <cite>0.5</cite>, <cite>1.5</cite>, <cite>2.5</cite> or
<cite>inf</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>While for <cite>nu = float(‘inf’)</cite> this kernel is equivalent to the <a class="reference internal" href="#falkon.kernels.GaussianKernel" title="falkon.kernels.GaussianKernel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GaussianKernel</span></code></a>,
this implementation is more general. Using the <a class="reference internal" href="#falkon.kernels.GaussianKernel" title="falkon.kernels.GaussianKernel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GaussianKernel</span></code></a> directly
may be computationally more efficient.</p>
<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.MaternKernel.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diag</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#falkon.kernels.MaternKernel.__call__" title="Permalink to this definition"></a></dt>
<dd><p>Compute the kernel matrix between <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <code class="docutils literal notranslate"><span class="pre">X2</span> <span class="pre">==</span> <span class="pre">X1</span></code> to compute a symmetric kernel.</p></li>
<li><p><strong>diag</strong> (<em>bool</em>) – Whether to compute just the diagonal of the kernel matrix, or the whole matrix.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (N x M) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options to be used for computing the operation. Useful are the memory size options
and CUDA options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The kernel between <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.MaternKernel.dmmv">
<span class="sig-name descname"><span class="pre">dmmv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">w</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.MaternKernel.dmmv" title="Permalink to this definition"></a></dt>
<dd><p>Compute double matrix-vector multiplications where the matrix is the current kernel.</p>
<p>The general form of <cite>dmmv</cite> operations is: <cite>Kernel(X2, X1) &#64; (Kernel(X1, X2) &#64; v + w)</cite>
where if <cite>v</cite> is None, then we simply have <cite>Kernel(X2, X1) &#64; w</cite> and if <cite>w</cite> is None
we remove the additive factor.
<strong>At least one of `w` and `v` must be provided</strong>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <cite>X2 == X1</cite> to compute a symmetric kernel.</p></li>
<li><p><strong>v</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – A vector to compute the matrix-vector product. This may also be a matrix of shape
(M x T), but if <cite>T</cite> is very large the operations will be much slower.</p></li>
<li><p><strong>w</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – A vector to compute matrix-vector products. This may also be a matrix of shape
(N x T) but if <cite>T</cite> is very large the operations will be much slower.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (M x T) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options to be used for computing the operation. Useful are the memory size options
and CUDA options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">X1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">X1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">X2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">X2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The (M x T) output.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">falkon</span><span class="o">,</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">falkon</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">GaussianKernel</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># You can substitute the Gaussian kernel by any other.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># N is 100, D is 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># M is 150</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">dmmv</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([150, 1])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.MaternKernel.mmv">
<span class="sig-name descname"><span class="pre">mmv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.MaternKernel.mmv" title="Permalink to this definition"></a></dt>
<dd><p>Compute matrix-vector multiplications where the matrix is the current kernel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <cite>X2 == X1</cite> to compute a symmetric kernel.</p></li>
<li><p><strong>v</strong> (<em>torch.Tensor</em>) – A vector to compute the matrix-vector product. This may also be a matrix of shape
(M x T), but if <cite>T</cite> is very large the operations will be much slower.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (N x T) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options to be used for computing the operation. Useful are the memory size options
and CUDA options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The (N x T) output.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">falkon</span><span class="o">,</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">falkon</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">GaussianKernel</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># You can substitute the Gaussian kernel by any other.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">mmv</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([100, 1])</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="dot-product-kernels">
<h2>Dot-Product kernels<a class="headerlink" href="#dot-product-kernels" title="Permalink to this heading"></a></h2>
<section id="polynomial-kernel">
<h3>Polynomial kernel<a class="headerlink" href="#polynomial-kernel" title="Permalink to this heading"></a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="falkon.kernels.PolynomialKernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">falkon.kernels.</span></span><span class="sig-name descname"><span class="pre">PolynomialKernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">degree</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.PolynomialKernel" title="Permalink to this definition"></a></dt>
<dd><p>Polynomial kernel with multiplicative and additive constants.</p>
<p>Follows the formula</p>
<div class="math notranslate nohighlight">
\[(\gamma * X_1^\top X_2 + \beta)^{\mathrm{degree}}\]</div>
<p>Where all operations apart from the matrix multiplication are taken element-wise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>beta</strong> (<em>float-like</em>) – Additive constant</p></li>
<li><p><strong>gamma</strong> (<em>float-like</em>) – Multiplicative constant</p></li>
<li><p><strong>degree</strong> (<em>float-like</em>) – Power of the polynomial kernel</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options which will be used in downstream kernel operations.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.PolynomialKernel.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diag</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#falkon.kernels.PolynomialKernel.__call__" title="Permalink to this definition"></a></dt>
<dd><p>Compute the kernel matrix between <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <code class="docutils literal notranslate"><span class="pre">X2</span> <span class="pre">==</span> <span class="pre">X1</span></code> to compute a symmetric kernel.</p></li>
<li><p><strong>diag</strong> (<em>bool</em>) – Whether to compute just the diagonal of the kernel matrix, or the whole matrix.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (N x M) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options to be used for computing the operation. Useful are the memory size options
and CUDA options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The kernel between <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.PolynomialKernel.dmmv">
<span class="sig-name descname"><span class="pre">dmmv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">w</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.PolynomialKernel.dmmv" title="Permalink to this definition"></a></dt>
<dd><p>Compute double matrix-vector multiplications where the matrix is the current kernel.</p>
<p>The general form of <cite>dmmv</cite> operations is: <cite>Kernel(X2, X1) &#64; (Kernel(X1, X2) &#64; v + w)</cite>
where if <cite>v</cite> is None, then we simply have <cite>Kernel(X2, X1) &#64; w</cite> and if <cite>w</cite> is None
we remove the additive factor.
<strong>At least one of `w` and `v` must be provided</strong>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <cite>X2 == X1</cite> to compute a symmetric kernel.</p></li>
<li><p><strong>v</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – A vector to compute the matrix-vector product. This may also be a matrix of shape
(M x T), but if <cite>T</cite> is very large the operations will be much slower.</p></li>
<li><p><strong>w</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – A vector to compute matrix-vector products. This may also be a matrix of shape
(N x T) but if <cite>T</cite> is very large the operations will be much slower.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (M x T) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options to be used for computing the operation. Useful are the memory size options
and CUDA options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">X1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">X1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">X2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">X2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The (M x T) output.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">falkon</span><span class="o">,</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">falkon</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">GaussianKernel</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># You can substitute the Gaussian kernel by any other.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># N is 100, D is 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># M is 150</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">dmmv</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([150, 1])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.PolynomialKernel.mmv">
<span class="sig-name descname"><span class="pre">mmv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.PolynomialKernel.mmv" title="Permalink to this definition"></a></dt>
<dd><p>Compute matrix-vector multiplications where the matrix is the current kernel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <cite>X2 == X1</cite> to compute a symmetric kernel.</p></li>
<li><p><strong>v</strong> (<em>torch.Tensor</em>) – A vector to compute the matrix-vector product. This may also be a matrix of shape
(M x T), but if <cite>T</cite> is very large the operations will be much slower.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (N x T) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options to be used for computing the operation. Useful are the memory size options
and CUDA options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The (N x T) output.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">falkon</span><span class="o">,</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">falkon</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">GaussianKernel</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># You can substitute the Gaussian kernel by any other.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">mmv</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([100, 1])</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="linear-kernel">
<h3>Linear kernel<a class="headerlink" href="#linear-kernel" title="Permalink to this heading"></a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="falkon.kernels.LinearKernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">falkon.kernels.</span></span><span class="sig-name descname"><span class="pre">LinearKernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.LinearKernel" title="Permalink to this definition"></a></dt>
<dd><p>Linear Kernel with optional scaling and translation parameters.</p>
<p>The kernel implemented here is the covariance function in the original
input space (i.e. <cite>X &#64; X.T</cite>) with optional parameters to translate
and scale the kernel: <cite>beta + gamma * X &#64; X.T</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>beta</strong> (<em>float-like</em>) – Additive constant for the kernel, default: 0.0</p></li>
<li><p><strong>gamma</strong> (<em>float-like</em>) – Multiplicative constant for the kernel. The kernel will
be multiplied by the inverse of sigma squared. Default: 1.0</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options which will be used in downstream kernel operations.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">LinearKernel</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># 100 samples in 3 dimensions</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kernel_matrix</span> <span class="o">=</span> <span class="n">k</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span><span class="n">kernel_matrix</span><span class="p">,</span> <span class="n">X</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.LinearKernel.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diag</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#falkon.kernels.LinearKernel.__call__" title="Permalink to this definition"></a></dt>
<dd><p>Compute the kernel matrix between <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <code class="docutils literal notranslate"><span class="pre">X2</span> <span class="pre">==</span> <span class="pre">X1</span></code> to compute a symmetric kernel.</p></li>
<li><p><strong>diag</strong> (<em>bool</em>) – Whether to compute just the diagonal of the kernel matrix, or the whole matrix.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (N x M) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options to be used for computing the operation. Useful are the memory size options
and CUDA options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The kernel between <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.LinearKernel.dmmv">
<span class="sig-name descname"><span class="pre">dmmv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">w</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.LinearKernel.dmmv" title="Permalink to this definition"></a></dt>
<dd><p>Compute double matrix-vector multiplications where the matrix is the current kernel.</p>
<p>The general form of <cite>dmmv</cite> operations is: <cite>Kernel(X2, X1) &#64; (Kernel(X1, X2) &#64; v + w)</cite>
where if <cite>v</cite> is None, then we simply have <cite>Kernel(X2, X1) &#64; w</cite> and if <cite>w</cite> is None
we remove the additive factor.
<strong>At least one of `w` and `v` must be provided</strong>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <cite>X2 == X1</cite> to compute a symmetric kernel.</p></li>
<li><p><strong>v</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – A vector to compute the matrix-vector product. This may also be a matrix of shape
(M x T), but if <cite>T</cite> is very large the operations will be much slower.</p></li>
<li><p><strong>w</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – A vector to compute matrix-vector products. This may also be a matrix of shape
(N x T) but if <cite>T</cite> is very large the operations will be much slower.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (M x T) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options to be used for computing the operation. Useful are the memory size options
and CUDA options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">X1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">X1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">X2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">X2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The (M x T) output.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">falkon</span><span class="o">,</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">falkon</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">GaussianKernel</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># You can substitute the Gaussian kernel by any other.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># N is 100, D is 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># M is 150</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">dmmv</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([150, 1])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.LinearKernel.mmv">
<span class="sig-name descname"><span class="pre">mmv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.LinearKernel.mmv" title="Permalink to this definition"></a></dt>
<dd><p>Compute matrix-vector multiplications where the matrix is the current kernel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <cite>X2 == X1</cite> to compute a symmetric kernel.</p></li>
<li><p><strong>v</strong> (<em>torch.Tensor</em>) – A vector to compute the matrix-vector product. This may also be a matrix of shape
(M x T), but if <cite>T</cite> is very large the operations will be much slower.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (N x T) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options to be used for computing the operation. Useful are the memory size options
and CUDA options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The (N x T) output.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">falkon</span><span class="o">,</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">falkon</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">GaussianKernel</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># You can substitute the Gaussian kernel by any other.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">mmv</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([100, 1])</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="sigmoid-kernel">
<h3>Sigmoid kernel<a class="headerlink" href="#sigmoid-kernel" title="Permalink to this heading"></a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="falkon.kernels.SigmoidKernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">falkon.kernels.</span></span><span class="sig-name descname"><span class="pre">SigmoidKernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.SigmoidKernel" title="Permalink to this definition"></a></dt>
<dd><p>Sigmoid (or hyperbolic tangent) kernel function, with additive and multiplicative constants.</p>
<p>Follows the formula</p>
<div class="math notranslate nohighlight">
\[k(x, y) = \tanh(\alpha x^\top y + \beta)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>beta</strong> (<em>float-like</em>) – Multiplicative constant</p></li>
<li><p><strong>gamma</strong> (<em>float-like</em>) – Multiplicative constant</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options which will be used in downstream kernel operations.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.SigmoidKernel.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diag</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#falkon.kernels.SigmoidKernel.__call__" title="Permalink to this definition"></a></dt>
<dd><p>Compute the kernel matrix between <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <code class="docutils literal notranslate"><span class="pre">X2</span> <span class="pre">==</span> <span class="pre">X1</span></code> to compute a symmetric kernel.</p></li>
<li><p><strong>diag</strong> (<em>bool</em>) – Whether to compute just the diagonal of the kernel matrix, or the whole matrix.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (N x M) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options to be used for computing the operation. Useful are the memory size options
and CUDA options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The kernel between <code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.SigmoidKernel.dmmv">
<span class="sig-name descname"><span class="pre">dmmv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">w</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.SigmoidKernel.dmmv" title="Permalink to this definition"></a></dt>
<dd><p>Compute double matrix-vector multiplications where the matrix is the current kernel.</p>
<p>The general form of <cite>dmmv</cite> operations is: <cite>Kernel(X2, X1) &#64; (Kernel(X1, X2) &#64; v + w)</cite>
where if <cite>v</cite> is None, then we simply have <cite>Kernel(X2, X1) &#64; w</cite> and if <cite>w</cite> is None
we remove the additive factor.
<strong>At least one of `w` and `v` must be provided</strong>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <cite>X2 == X1</cite> to compute a symmetric kernel.</p></li>
<li><p><strong>v</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – A vector to compute the matrix-vector product. This may also be a matrix of shape
(M x T), but if <cite>T</cite> is very large the operations will be much slower.</p></li>
<li><p><strong>w</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – A vector to compute matrix-vector products. This may also be a matrix of shape
(N x T) but if <cite>T</cite> is very large the operations will be much slower.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (M x T) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options to be used for computing the operation. Useful are the memory size options
and CUDA options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">X1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">X1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">X2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">X2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The (M x T) output.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">falkon</span><span class="o">,</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">falkon</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">GaussianKernel</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># You can substitute the Gaussian kernel by any other.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># N is 100, D is 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># M is 150</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">dmmv</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([150, 1])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="falkon.kernels.SigmoidKernel.mmv">
<span class="sig-name descname"><span class="pre">mmv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">X2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor"><span class="pre">SparseTensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><span class="pre">FalkonOptions</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_m2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.kernels.SigmoidKernel.mmv" title="Permalink to this definition"></a></dt>
<dd><p>Compute matrix-vector multiplications where the matrix is the current kernel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> (<em>torch.Tensor</em>) – The first data-matrix for computing the kernel. Of shape (N x D):
N samples in D dimensions.</p></li>
<li><p><strong>X2</strong> (<em>torch.Tensor</em>) – The second data-matrix for computing the kernel. Of shape (M x D):
M samples in D dimensions. Set <cite>X2 == X1</cite> to compute a symmetric kernel.</p></li>
<li><p><strong>v</strong> (<em>torch.Tensor</em>) – A vector to compute the matrix-vector product. This may also be a matrix of shape
(M x T), but if <cite>T</cite> is very large the operations will be much slower.</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – Optional tensor of shape (N x T) to hold the output. If not provided it will
be created.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions"><em>FalkonOptions</em></a><em>]</em>) – Options to be used for computing the operation. Useful are the memory size options
and CUDA options.</p></li>
<li><p><strong>kwargs_m1</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m1</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m1</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
<li><p><strong>kwargs_m2</strong> – Keyword arguments containing tensors which should be split along with <code class="docutils literal notranslate"><span class="pre">m2</span></code>.
For example this could be a set of indices corresponding to <code class="docutils literal notranslate"><span class="pre">m2</span></code>, which are then
correctly split and available in the kernel computation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> (<em>torch.Tensor</em>) – The (N x T) output.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">falkon</span><span class="o">,</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">falkon</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">GaussianKernel</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># You can substitute the Gaussian kernel by any other.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">mmv</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([100, 1])</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="models.html" class="btn btn-neutral float-left" title="falkon.models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="options.html" class="btn btn-neutral float-right" title="falkon.options" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Giacomo Meanti, Alessandro Rudi.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>