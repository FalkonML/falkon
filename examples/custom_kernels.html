<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Implementing A Custom Kernel &mdash; falkon 0.7.5 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Automatic Hyperparameter Optimization" href="hyperopt.html" />
    <link rel="prev" title="Hyperparameter Tuning with Falkon" href="falkon_cv.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> falkon
          </a>
              <div class="version">
                0.7.5
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../install.html">Install</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../install.html#supported-platforms">Supported Platforms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#prerequisites">Prerequisites</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../install.html#pytorch-and-cuda">PyTorch and CUDA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../install.html#intel-mkl">Intel MKL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#installing">Installing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#testing-the-installation">Testing the installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#development">Development</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../get_started.html">Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../get_started.html#passing-options">Passing Options</a></li>
<li class="toctree-l2"><a class="reference internal" href="../get_started.html#more-examples">More Examples</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="examples.html">Examples</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="falkon_regression_tutorial.html">Falkon Regression Tutorial</a><ul>
<li class="toctree-l3"><a class="reference internal" href="falkon_regression_tutorial.html#Introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="falkon_regression_tutorial.html#Load-the-data">Load the data</a></li>
<li class="toctree-l3"><a class="reference internal" href="falkon_regression_tutorial.html#Pre-process-the-data">Pre-process the data</a></li>
<li class="toctree-l3"><a class="reference internal" href="falkon_regression_tutorial.html#Create-the-Falkon-model">Create the Falkon model</a></li>
<li class="toctree-l3"><a class="reference internal" href="falkon_regression_tutorial.html#Training-the-model">Training the model</a></li>
<li class="toctree-l3"><a class="reference internal" href="falkon_regression_tutorial.html#Evaluating-model-performance">Evaluating model performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="logistic_falkon.html">Introducing Logistic Falkon</a><ul>
<li class="toctree-l3"><a class="reference internal" href="logistic_falkon.html#Introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="logistic_falkon.html#Load-the-data">Load the data</a></li>
<li class="toctree-l3"><a class="reference internal" href="logistic_falkon.html#Split-into-training-and-test-sets">Split into training and test sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="logistic_falkon.html#Data-Preprocessing">Data Preprocessing</a></li>
<li class="toctree-l3"><a class="reference internal" href="logistic_falkon.html#Define-the-Falkon-model">Define the Falkon model</a></li>
<li class="toctree-l3"><a class="reference internal" href="logistic_falkon.html#Define-Logistic-Falkon-model">Define Logistic Falkon model</a></li>
<li class="toctree-l3"><a class="reference internal" href="logistic_falkon.html#Train-both-models">Train both models</a></li>
<li class="toctree-l3"><a class="reference internal" href="logistic_falkon.html#Testing">Testing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="logistic_falkon.html#Plot-predictions">Plot predictions</a></li>
<li class="toctree-l2"><a class="reference internal" href="falkon_cv.html">Hyperparameter Tuning with Falkon</a><ul>
<li class="toctree-l3"><a class="reference internal" href="falkon_cv.html#Introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="falkon_cv.html#Load-the-data">Load the data</a></li>
<li class="toctree-l3"><a class="reference internal" href="falkon_cv.html#Split-into-training-and-test-sets">Split into training and test sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="falkon_cv.html#Data-Preprocessing">Data Preprocessing</a></li>
<li class="toctree-l3"><a class="reference internal" href="falkon_cv.html#Search-for-the-optimal-parameters">Search for the optimal parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="falkon_cv.html#Evaluating-the-model">Evaluating the model</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="falkon_cv.html#Plot-grid-search-results">Plot grid-search results</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Implementing A Custom Kernel</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Setup-a-simple-problem-for-testing">Setup a simple problem for testing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Basic-Kernel-Implementation">Basic Kernel Implementation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Test-the-basic-kernel">Test the basic kernel</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Differentiable-Kernel">Differentiable Kernel</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Test-the-differentiable-kernel">Test the differentiable kernel</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Adding-KeOps-Support">Adding KeOps Support</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Test-the-KeOps-kernel">Test the KeOps kernel</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Supporting-Sparse-Data">Supporting Sparse Data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Testing-sparse-support">Testing sparse support</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="hyperopt.html">Automatic Hyperparameter Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="hyperopt.html#Load-the-data">Load the data</a></li>
<li class="toctree-l3"><a class="reference internal" href="hyperopt.html#Split-into-training-and-test-sets">Split into training and test sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="hyperopt.html#Data-Preprocessing">Data Preprocessing</a></li>
<li class="toctree-l3"><a class="reference internal" href="hyperopt.html#Hyperparameter-Optimization">Hyperparameter Optimization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="falkon_mnist.html">MNIST Classification with Falkon</a><ul>
<li class="toctree-l3"><a class="reference internal" href="falkon_mnist.html#Download-the-MNIST-dataset-&amp;-load-it-in-memory">Download the MNIST dataset &amp; load it in memory</a></li>
<li class="toctree-l3"><a class="reference internal" href="falkon_mnist.html#Data-Preprocessing">Data Preprocessing</a></li>
<li class="toctree-l3"><a class="reference internal" href="falkon_mnist.html#Run-Falkon">Run Falkon</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/index.html">API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/models.html">falkon.models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/models.html#falkon">Falkon</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/models.html#falkon.models.Falkon"><code class="docutils literal notranslate"><span class="pre">Falkon</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/models.html#logisticfalkon">LogisticFalkon</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/models.html#falkon.models.LogisticFalkon"><code class="docutils literal notranslate"><span class="pre">LogisticFalkon</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/models.html#incorefalkon">InCoreFalkon</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/models.html#falkon.models.InCoreFalkon"><code class="docutils literal notranslate"><span class="pre">InCoreFalkon</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/kernels.html">falkon.kernels</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/kernels.html#kernel">Kernel</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/kernels.html#falkon.kernels.kernel.Kernel"><code class="docutils literal notranslate"><span class="pre">Kernel</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/kernels.html#diffkernel">DiffKernel</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/kernels.html#falkon.kernels.diff_kernel.DiffKernel"><code class="docutils literal notranslate"><span class="pre">DiffKernel</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/kernels.html#keopskernelmixin">KeopsKernelMixin</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/kernels.html#falkon.kernels.keops_helpers.KeopsKernelMixin"><code class="docutils literal notranslate"><span class="pre">KeopsKernelMixin</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/kernels.html#radial-kernels">Radial kernels</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/kernels.html#gaussian-kernel">Gaussian kernel</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/kernels.html#laplacian-kernel">Laplacian kernel</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/kernels.html#matern-kernel">Matern kernel</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/kernels.html#dot-product-kernels">Dot-Product kernels</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/kernels.html#polynomial-kernel">Polynomial kernel</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/kernels.html#linear-kernel">Linear kernel</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/kernels.html#sigmoid-kernel">Sigmoid kernel</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/options.html">falkon.options</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/options.html#falkonoptions">FalkonOptions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/options.html#falkon.options.FalkonOptions"><code class="docutils literal notranslate"><span class="pre">FalkonOptions</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/gsc_losses.html">falkon.gsc_losses</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/gsc_losses.html#loss">Loss</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/gsc_losses.html#falkon.gsc_losses.Loss"><code class="docutils literal notranslate"><span class="pre">Loss</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/gsc_losses.html#logistic-loss">Logistic loss</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/gsc_losses.html#falkon.gsc_losses.LogisticLoss"><code class="docutils literal notranslate"><span class="pre">LogisticLoss</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/gsc_losses.html#weighted-binary-cross-entropy-loss">Weighted binary cross entropy loss</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/gsc_losses.html#falkon.gsc_losses.WeightedCrossEntropyLoss"><code class="docutils literal notranslate"><span class="pre">WeightedCrossEntropyLoss</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/preconditioner.html">falkon.preconditioner</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/preconditioner.html#preconditioner">Preconditioner</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/preconditioner.html#falkon.preconditioner.preconditioner.Preconditioner"><code class="docutils literal notranslate"><span class="pre">Preconditioner</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/preconditioner.html#cholesky-preconditioners">Cholesky preconditioners</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/preconditioner.html#falkonpreconditioner">FalkonPreconditioner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/preconditioner.html#logisticpreconditioner">LogisticPreconditioner</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/optimization.html">falkon.optim</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/optimization.html#optimizer">Optimizer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/optimization.html#falkon.optim.Optimizer"><code class="docutils literal notranslate"><span class="pre">Optimizer</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/optimization.html#conjugate-gradient-methods">Conjugate gradient methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/optimization.html#conjugategradient">ConjugateGradient</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/optimization.html#falkonconjugategradient">FalkonConjugateGradient</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/outofcore.html">falkon.ooc_ops</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/outofcore.html#gpu-cholesky">gpu_cholesky</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/outofcore.html#falkon.ooc_ops.gpu_cholesky"><code class="docutils literal notranslate"><span class="pre">gpu_cholesky()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/outofcore.html#gpu-lauum">gpu_lauum</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/outofcore.html#falkon.ooc_ops.gpu_lauum"><code class="docutils literal notranslate"><span class="pre">gpu_lauum()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/mmv_ops.html">falkon.mmv_ops</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/mmv_ops.html#run-keops-mmv">run_keops_mmv</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/mmv_ops.html#falkon.mmv_ops.keops.run_keops_mmv"><code class="docutils literal notranslate"><span class="pre">run_keops_mmv()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/mmv_ops.html#fmm">fmm</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/mmv_ops.html#falkon.mmv_ops.fmm.fmm"><code class="docutils literal notranslate"><span class="pre">fmm()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/mmv_ops.html#fmmv">fmmv</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/mmv_ops.html#falkon.mmv_ops.fmmv.fmmv"><code class="docutils literal notranslate"><span class="pre">fmmv()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/mmv_ops.html#fdmmv">fdmmv</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/mmv_ops.html#falkon.mmv_ops.fmmv.fdmmv"><code class="docutils literal notranslate"><span class="pre">fdmmv()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/mmv_ops.html#incore-fmmv">incore_fmmv</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/mmv_ops.html#falkon.mmv_ops.fmmv_incore.incore_fmmv"><code class="docutils literal notranslate"><span class="pre">incore_fmmv()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/mmv_ops.html#incore-fdmmv">incore_fdmmv</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/mmv_ops.html#falkon.mmv_ops.fmmv_incore.incore_fdmmv"><code class="docutils literal notranslate"><span class="pre">incore_fdmmv()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/mmv_ops.html#low-level-functions">Low-level functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/mmv_ops.html#falkon.mmv_ops.fmm.sparse_mm_run_thread"><code class="docutils literal notranslate"><span class="pre">sparse_mm_run_thread()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/mmv_ops.html#falkon.mmv_ops.fmmv.sparse_mmv_run_thread"><code class="docutils literal notranslate"><span class="pre">sparse_mmv_run_thread()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/sparse.html">falkon.sparse</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/sparse.html#sparsetensor">SparseTensor</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/sparse.html#falkon.sparse.sparse_tensor.SparseTensor"><code class="docutils literal notranslate"><span class="pre">SparseTensor</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/sparse.html#falkon.sparse.sparse_tensor.SparseType"><code class="docutils literal notranslate"><span class="pre">SparseType</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/sparse.html#sparse-operations">Sparse operations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/sparse.html#falkon.sparse.sparse_matmul"><code class="docutils literal notranslate"><span class="pre">sparse_matmul()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/sparse.html#falkon.sparse.sparse_square_norm"><code class="docutils literal notranslate"><span class="pre">sparse_square_norm()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/sparse.html#falkon.sparse.sparse_norm"><code class="docutils literal notranslate"><span class="pre">sparse_norm()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/center_selector.html">falkon.center_selection</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/center_selector.html#centerselector">CenterSelector</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/center_selector.html#falkon.center_selection.CenterSelector"><code class="docutils literal notranslate"><span class="pre">CenterSelector</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/center_selector.html#uniformselector">UniformSelector</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/center_selector.html#falkon.center_selection.UniformSelector"><code class="docutils literal notranslate"><span class="pre">UniformSelector</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/center_selector.html#fixedselector">FixedSelector</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/center_selector.html#falkon.center_selection.FixedSelector"><code class="docutils literal notranslate"><span class="pre">FixedSelector</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/hopt.html">falkon.hopt</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/hopt.html#objectives">Objectives</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/hopt.html#falkon.hopt.objectives.objectives.HyperoptObjective"><code class="docutils literal notranslate"><span class="pre">HyperoptObjective</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/hopt.html#nystrom-complexity-regularization">Nystrom Complexity Regularization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/hopt.html#stochastic-nystrom-computational-regularization">Stochastic Nystrom Computational Regularization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/hopt.html#complexity-regularization">Complexity Regularization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/hopt.html#generalized-cross-validation">Generalized Cross Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/hopt.html#hold-out-cross-validation">Hold Out Cross Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/hopt.html#leave-one-out-cross-validation">Leave One Out Cross Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/hopt.html#sgpr">SGPR</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">falkon</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a></li>
          <li class="breadcrumb-item"><a href="examples.html">Examples</a></li>
      <li class="breadcrumb-item active">Implementing A Custom Kernel</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/examples/custom_kernels.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars and line breaks on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
    white-space: pre;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="Implementing-A-Custom-Kernel">
<h1>Implementing A Custom Kernel<a class="headerlink" href="#Implementing-A-Custom-Kernel" title="Permalink to this heading"></a></h1>
<p>In this notebook we will show how to implement a custom kernel in Falkon.</p>
<p>There are several complementary parts to a kernel, which can be added to support different operations. We will go through them one-by-one in this notebook:</p>
<ul class="simple">
<li><p>Basic support: supports learning with Falkon!</p></li>
<li><p>Autodiff support: supports automatic hyperparameter tuning (in the <code class="docutils literal notranslate"><span class="pre">hopt</span></code> module)</p></li>
<li><p>KeOps support: faster kernel-vector products in low dimension</p></li>
<li><p>Sparse support: support learning on sparse data.</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import matplotlib.pyplot as plt
plt.style.use(&#39;ggplot&#39;)

from sklearn import datasets
import torch
import numpy as np

import falkon
from falkon import FalkonOptions
from falkon.kernels import Kernel, DiffKernel, KeopsKernelMixin
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[pyKeOps]: Warning, no cuda detected. Switching to cpu only.
</pre></div></div>
</div>
<section id="Setup-a-simple-problem-for-testing">
<h2>Setup a simple problem for testing<a class="headerlink" href="#Setup-a-simple-problem-for-testing" title="Permalink to this heading"></a></h2>
<p>Load and preprocess the <em>California housing</em> dataset. The <code class="docutils literal notranslate"><span class="pre">learn_with_kernel</span></code> function sets up Falkon for learning on the California housing datase with a given kernel.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X, Y = datasets.fetch_california_housing(return_X_y=True)
num_train = int(X.shape[0] * 0.8)
num_test = X.shape[0] - num_train
shuffle_idx = np.arange(X.shape[0])
np.random.shuffle(shuffle_idx)
train_idx = shuffle_idx[:num_train]
test_idx = shuffle_idx[num_train:]

Xtrain, Ytrain = X[train_idx], Y[train_idx]
Xtest, Ytest = X[test_idx], Y[test_idx]
# convert numpy -&gt; pytorch
Xtrain = torch.from_numpy(Xtrain).to(dtype=torch.float32)
Xtest = torch.from_numpy(Xtest).to(dtype=torch.float32)
Ytrain = torch.from_numpy(Ytrain).to(dtype=torch.float32)
Ytest = torch.from_numpy(Ytest).to(dtype=torch.float32)
# z-score normalization
train_mean = Xtrain.mean(0, keepdim=True)
train_std = Xtrain.std(0, keepdim=True)
Xtrain -= train_mean
Xtrain /= train_std
Xtest -= train_mean
Xtest /= train_std
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def rmse(true, pred):
    return torch.sqrt(torch.mean((true.reshape(-1, 1) - pred.reshape(-1, 1))**2))

def learn_with_kernel(kernel):
    flk_opt = FalkonOptions(use_cpu=True)
    model = falkon.Falkon(
        kernel=kernel, penalty=1e-5, M=1000, options=flk_opt,
        error_every=1, error_fn=rmse)
    model.fit(Xtrain, Ytrain)
    ts_err = rmse(Ytest, model.predict(Xtest))
    print(&quot;Test RMSE: %.2f&quot; % (ts_err))
</pre></div>
</div>
</div>
</section>
<section id="Basic-Kernel-Implementation">
<h2>Basic Kernel Implementation<a class="headerlink" href="#Basic-Kernel-Implementation" title="Permalink to this heading"></a></h2>
<p>We must inherit from the <code class="docutils literal notranslate"><span class="pre">falkon.kernels.Kernel</span></code> class, and implement: - <code class="docutils literal notranslate"><span class="pre">compute</span></code> method: the core of the kernel implementation. Given two input matrices (of size <span class="math notranslate nohighlight">\(n\times d\)</span> and <span class="math notranslate nohighlight">\(m\times d\)</span>), and an output matrix (of size <span class="math notranslate nohighlight">\(n\times m\)</span>), compute the kernel function between the two inputs and store it in the output.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>The additional `diag` parameter is a boolean flag. It indicates that a) $n$ is equal to $m$, b) only the diagonal of the kernel matrix should be computed.
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">compute_sparse</span></code> method: this should be used if you want your kernel to support sparse data. We will implement it in a later section.</p></li>
</ul>
<p>We will implement a <strong>linear</strong> kernel:</p>
<div class="math notranslate nohighlight">
\[k(x, x') = \sigma (x^\top x')\]</div>
<p>the parameter <span class="math notranslate nohighlight">\(\sigma\)</span> is the <em>variance</em> of the kernel. It is the only hyperparameter.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class BasicLinearKernel(Kernel):
    def __init__(self, lengthscale, options):
        # The base class takes as inputs a name for the kernel, and
        # an instance of `FalkonOptions`.
        super().__init__(&quot;basic_linear&quot;, options)

        self.lengthscale = lengthscale

    def compute(self, X1: torch.Tensor, X2: torch.Tensor, out: torch.Tensor, diag: bool) -&gt; torch.Tensor:
        # To support different devices/data types, you must make sure
        # the lengthscale is compatible with the data.
        lengthscale = self.lengthscale.to(device=X1.device, dtype=X1.dtype)

        scaled_X1 = X1 * lengthscale

        if diag:
            out.copy_(torch.sum(scaled_X1 * X2, dim=-1))
        else:
            # The dot-product row-by-row on `X1` and `X2` can be computed
            # on many rows at a time with matrix multiplication.
            out = torch.matmul(scaled_X1, X2.T, out=out)

        return out

    def compute_sparse(self, X1, X2, out, diag, **kwargs) -&gt; torch.Tensor:
        raise NotImplementedError(&quot;Sparse not implemented&quot;)
</pre></div>
</div>
</div>
<section id="Test-the-basic-kernel">
<h3>Test the basic kernel<a class="headerlink" href="#Test-the-basic-kernel" title="Permalink to this heading"></a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Initialize the kernel
lengthscale_init = torch.tensor([1.0])
k = BasicLinearKernel(lengthscale_init, options=falkon.FalkonOptions())
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># The kernel matrix
k(torch.randn(5, 3), torch.randn(5, 3))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor([[-1.3538,  4.0383, -0.5058, -3.1306, -0.3159],
        [-0.9498, -2.0581,  0.4684,  0.8994,  0.7577],
        [ 0.3122, -0.1038, -0.5039,  2.5076, -0.4032],
        [ 0.8383,  3.8545, -1.4094,  1.0497, -1.4979],
        [ 0.8344, -4.5258,  2.9362, -7.7300,  2.0740]])
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Kernel-vector product
k.mmv(torch.randn(4, 3), torch.randn(4, 3), v=torch.randn(4, 1))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor([[6.1084],
        [3.6743],
        [1.2653],
        [1.2448]])
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Double kernel-vector product
k.dmmv(torch.randn(3, 3), torch.randn(4, 3), v=torch.randn(4, 1), w=torch.randn(3, 1))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor([[ -3.6467],
        [ -9.8628],
        [  1.4857],
        [-12.8557]])
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Learning on the california housing dataset
learn_with_kernel(k)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Iteration   1 - Elapsed 0.07s - training error: 2.36367178
Iteration   2 - Elapsed 0.11s - training error: 2.19508219
Iteration   3 - Elapsed 0.14s - training error: 2.19265079
Iteration   4 - Elapsed 0.17s - training error: 2.19265032
Iteration   5 - Elapsed 0.20s - training error: 2.19262338
Iteration   6 - Elapsed 0.24s - training error: 2.19262123
Iteration   7 - Elapsed 0.27s - training error: 2.19261861
Iteration   8 - Elapsed 0.30s - training error: 2.19261885
Iteration   9 - Elapsed 0.33s - training error: 2.19261789
Iteration  10 - Elapsed 0.39s - training error: 2.19261765
Iteration  11 - Elapsed 0.42s - training error: 2.19261956
Iteration  12 - Elapsed 0.45s - training error: 2.19261932
Iteration  13 - Elapsed 0.48s - training error: 2.19261909
Iteration  14 - Elapsed 0.51s - training error: 2.19261813
Iteration  15 - Elapsed 0.55s - training error: 2.19261885
Iteration  16 - Elapsed 0.57s - training error: 2.19261742
Iteration  17 - Elapsed 0.61s - training error: 2.19261813
Iteration  18 - Elapsed 0.63s - training error: 2.19261980
Iteration  19 - Elapsed 0.66s - training error: 2.19261956
Iteration  20 - Elapsed 0.73s - training error: 2.19262052
Test RMSE: 2.19
</pre></div></div>
</div>
</section>
</section>
<section id="Differentiable-Kernel">
<h2>Differentiable Kernel<a class="headerlink" href="#Differentiable-Kernel" title="Permalink to this heading"></a></h2>
<p>A differentiable kernel is needed for automatic hyperparameter optimization (see the <a class="reference internal" href="hyperopt.html"><span class="doc">notebook</span></a>).</p>
<p>It requires inheriting from <code class="docutils literal notranslate"><span class="pre">falkon.kernels.DiffKernel</span></code>. In addition to the methods already discussed, we must implement: - <code class="docutils literal notranslate"><span class="pre">compute_diff</span></code>, which works similarly to the <code class="docutils literal notranslate"><span class="pre">compute</span></code> method but it does not have an <code class="docutils literal notranslate"><span class="pre">out</span></code> parameter. The implementation should be fully differentiable with respect to its inputs, and to the kernel hyperparameters. - <code class="docutils literal notranslate"><span class="pre">detach</span></code>, which essentially clones the kernel with the parameters <em>detached</em> from the computational graph.</p>
<p>Another important difference from the basic kernel is the call to the <em>constructor</em>, which must include - All kernel hyperparameters as keyword arguments. These will be available as attributes on the class. Hyperparameters do not need to be tensors.</p>
<p><strong>``core_fn`` parameter (optional)</strong></p>
<p>The constructor can also <em>optionally</em> contain a <code class="docutils literal notranslate"><span class="pre">core_fn</span></code> parameter which can simplify implementation by uniting the <code class="docutils literal notranslate"><span class="pre">compute</span></code> and <code class="docutils literal notranslate"><span class="pre">compute_diff</span></code> implementations. Have a look at the implementation of kernels in <code class="docutils literal notranslate"><span class="pre">falkon.kernels.dot_prod_kernel.py</span></code> and <code class="docutils literal notranslate"><span class="pre">falkon.kernels.distance_kernel.py</span></code> for how to use the <code class="docutils literal notranslate"><span class="pre">core_fn</span></code> parameter.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class DiffLinearKernel(DiffKernel):
    def __init__(self, lengthscale, options):
        # Super-class constructor call. We do not specify core_fn
        # but we must specify the hyperparameter of this kernel (lengthscale)
        super().__init__(&quot;diff_linear&quot;,
                         options,
                         core_fn=None,
                         lengthscale=lengthscale)

    def compute(self, X1: torch.Tensor, X2: torch.Tensor, out: torch.Tensor, diag: bool):
        lengthscale = self.lengthscale.to(device=X1.device, dtype=X1.dtype)
        scaled_X1 = X1 * lengthscale
        if diag:
            out.copy_(torch.sum(scaled_X1 * X2, dim=-1))
        else:
            out = torch.matmul(scaled_X1, X2.T, out=out)

        return out

    def compute_diff(self, X1: torch.Tensor, X2: torch.Tensor, diag: bool):
        # The implementation here is similar to `compute` without in-place operations.
        lengthscale = self.lengthscale.to(device=X1.device, dtype=X1.dtype)
        scaled_X1 = X1 * lengthscale

        if diag:
            return torch.sum(scaled_X1 * X2, dim=-1)

        return torch.matmul(scaled_X1, X2.T)

    def detach(self):
        # Clones the class with detached hyperparameters
        return DiffLinearKernel(
            lengthscale=self.lengthscale.detach(),
            options=self.params
        )

    def compute_sparse(self, X1, X2, out, diag, **kwargs) -&gt; torch.Tensor:
        raise NotImplementedError(&quot;Sparse not implemented&quot;)
</pre></div>
</div>
</div>
<section id="Test-the-differentiable-kernel">
<h3>Test the differentiable kernel<a class="headerlink" href="#Test-the-differentiable-kernel" title="Permalink to this heading"></a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Initialize the kernel, with a lengthscale which requires grad.
lengthscale_init = torch.tensor([1.0]).requires_grad_()
k = DiffLinearKernel(lengthscale_init, options=falkon.FalkonOptions())
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Kernel matrix. Notice how the outputs has a `grad_fn`
k_mat = k(torch.randn(5, 3), torch.randn(5, 3))
k_mat
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor([[ 2.7480,  1.6149, -1.2979, -2.3070, -1.1852],
        [ 4.2437,  2.8397, -2.6248, -3.1610, -1.1940],
        [ 2.6474,  0.9644, -0.4447, -1.1742, -1.0197],
        [-3.4735,  0.4214, -1.9773,  0.3380,  2.2361],
        [-1.8094, -0.2183, -0.5620,  1.8260,  1.8644]],
       grad_fn=&lt;KernelMmFnFullBackward&gt;)
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Gradient of the kernel with respect to the lengthscale.
torch.autograd.grad(k_mat.sum(), k.lengthscale)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(tensor([-0.7049]),)
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># kernel-vector product + gradient
m1 = torch.randn(4, 3).requires_grad_()
m2 = torch.randn(2, 3)
v = torch.randn(2, 1)
k_mmv = k.mmv(m1, m2, v)
print(&quot;Kernel-vector product&quot;)
print(k_mmv)
print(&quot;Gradients:&quot;)
print(torch.autograd.grad(k_mmv.sum(), [k.lengthscale, m1]))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Kernel-vector product
tensor([[ 0.0198],
        [-1.6055],
        [ 2.3654],
        [-0.6039]], grad_fn=&lt;KernelMmvFnFullBackward&gt;)
Gradients:
(tensor([0.1758]), tensor([[ 0.6192,  1.2183, -0.2544],
        [ 0.6192,  1.2183, -0.2544],
        [ 0.6192,  1.2183, -0.2544],
        [ 0.6192,  1.2183, -0.2544]]))
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Learning on the california housing dataset
learn_with_kernel(k)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Iteration   1 - Elapsed 0.06s - training error: 2.20815659
Iteration   2 - Elapsed 0.10s - training error: 2.19324374
Iteration   3 - Elapsed 0.12s - training error: 2.19264197
Iteration   4 - Elapsed 0.15s - training error: 2.19263649
Iteration   5 - Elapsed 0.18s - training error: 2.19262934
Iteration   6 - Elapsed 0.21s - training error: 2.19261909
Iteration   7 - Elapsed 0.24s - training error: 2.19261813
Iteration   8 - Elapsed 0.26s - training error: 2.19262004
Iteration   9 - Elapsed 0.29s - training error: 2.19261765
Iteration  10 - Elapsed 0.34s - training error: 2.19261789
Iteration  11 - Elapsed 0.38s - training error: 2.19261909
Iteration  12 - Elapsed 0.40s - training error: 2.19261885
Iteration  13 - Elapsed 0.43s - training error: 2.19261956
Iteration  14 - Elapsed 0.46s - training error: 2.19261932
Iteration  15 - Elapsed 0.49s - training error: 2.19261932
Iteration  16 - Elapsed 0.52s - training error: 2.19262099
Iteration  17 - Elapsed 0.54s - training error: 2.19262123
Iteration  18 - Elapsed 0.57s - training error: 2.19262147
Iteration  19 - Elapsed 0.60s - training error: 2.19262195
Iteration  20 - Elapsed 0.65s - training error: 2.19262338
Test RMSE: 2.19
</pre></div></div>
</div>
</section>
</section>
<section id="Adding-KeOps-Support">
<h2>Adding KeOps Support<a class="headerlink" href="#Adding-KeOps-Support" title="Permalink to this heading"></a></h2>
<p>We must inherit from <code class="docutils literal notranslate"><span class="pre">falkon.kernels.KeopsKernelMixin</span></code> and implement the method <code class="docutils literal notranslate"><span class="pre">keops_mmv_impl</span></code>.</p>
<p>KeOps-enabled kernels will still use the implementation in the <code class="docutils literal notranslate"><span class="pre">compute</span></code> function for computing the kernel matrix itself, but will use KeOps to compute kernel-vector products (if the data dimension is small enough).</p>
<p>This method is responsible for kernel-vector products, and it should contain: 1. A formula definition (see <a class="reference external" href="https://www.kernel-operations.io/keops/api/math-operations.html">https://www.kernel-operations.io/keops/api/math-operations.html</a> for the appropriate syntax) 2. A definition of all variables (again have a look at the KeOps documentation, or the implementation of other kernels within Falkon) 3. A call to the <code class="docutils literal notranslate"><span class="pre">keops_mmv</span></code> method of the <code class="docutils literal notranslate"><span class="pre">KeopsKernelMixin</span></code> class, responsible for calling into the KeOps formula.</p>
<p>For our kernel we will use the <code class="docutils literal notranslate"><span class="pre">(X</span> <span class="pre">|</span> <span class="pre">Y)</span></code> syntax for the dot-product between samples, and then multiplication with the vector <code class="docutils literal notranslate"><span class="pre">v</span></code>. The aliases list maps the symbols used in the formula with the KeOps variable types.</p>
<p>For more examples check the <a class="reference external" href="https://www.kernel-operations.io">KeOps documentatiaon</a> or the implementation of existing kernels.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class KeopsLinearKernel(DiffKernel, KeopsKernelMixin):
    def __init__(self, lengthscale, options):
        super().__init__(&quot;my-keops-linear&quot;,
                         options,
                         core_fn=None,
                         lengthscale=lengthscale)

    def compute(self, X1: torch.Tensor, X2: torch.Tensor, out: torch.Tensor, diag: bool):
        lengthscale = self.lengthscale.to(device=X1.device, dtype=X1.dtype)
        scaled_X1 = X1 * lengthscale

        if diag:
            out.copy_(torch.sum(scaled_X1 * X2, dim=-1))
        else:
            out = torch.matmul(scaled_X1, X2.T, out=out)

        return out

    def compute_diff(self, X1: torch.Tensor, X2: torch.Tensor, diag: bool):
        scaled_X1 = X1 * self.lengthscale

        if diag:
            return torch.sum(scaled_X1 * X2, dim=-1)

        return torch.matmul(scaled_X1, X2.T)

    def detach(self):
        return KeopsLinearKernel(
            lengthscale=self.lengthscale.detach(),
            options=self.params
        )

    def keops_mmv_impl(self, X1, X2, v, kernel, out, opt):
        # Keops formula for kernel-vector.
        formula = &#39;(scale * (X | Y)) * v&#39;
        aliases = [
            &#39;X = Vi(%d)&#39; % (X1.shape[1]),
            &#39;Y = Vj(%d)&#39; % (X2.shape[1]),
            &#39;v = Vj(%d)&#39; % (v.shape[1]),
            &#39;scale = Pm(%d)&#39; % (self.lengthscale.shape[0]),
        ]
        other_vars = [
            self.lengthscale.to(dtype=X1.dtype, device=X1.device),
        ]
        # Call to the executor of the formula.
        return self.keops_mmv(X1, X2, v, out, formula, aliases, other_vars, opt)


    def compute_sparse(self, X1, X2, out: torch.Tensor, diag: bool, **kwargs) -&gt; torch.Tensor:
        raise NotImplementedError(&quot;Sparse not implemented&quot;)
</pre></div>
</div>
</div>
<section id="Test-the-KeOps-kernel">
<h3>Test the KeOps kernel<a class="headerlink" href="#Test-the-KeOps-kernel" title="Permalink to this heading"></a></h3>
<p>Note that KeOps will need to compile the kernels the first time they are run!</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>lengthscale_init = torch.tensor([1.0]).requires_grad_()
k = KeopsLinearKernel(lengthscale_init, options=falkon.FalkonOptions(use_cpu=True))
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># kernel-vector product + gradient
m1 = torch.randn(4, 3).requires_grad_()
m2 = torch.randn(2, 3)
v = torch.randn(2, 1)
k_mmv = k.mmv(m1, m2, v)
print(&quot;Kernel-vector product&quot;)
print(k_mmv)
print(&quot;Gradients:&quot;)
print(torch.autograd.grad(k_mmv.sum(), [k.lengthscale, m1]))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Kernel-vector product
tensor([[-1.2121],
        [-0.1148],
        [ 2.2435],
        [ 0.9918]], grad_fn=&lt;TilingGenredAutogradBackward&gt;)
Gradients:
(tensor([1.9084]), tensor([[ 1.0124, -0.8363,  0.7706],
        [ 1.0124, -0.8363,  0.7706],
        [ 1.0124, -0.8363,  0.7706],
        [ 1.0124, -0.8363,  0.7706]], requires_grad=True))
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Learning on the california housing dataset.
# Due to differences in floating point code, results may be slightly
# different from the other implementations.
learn_with_kernel(k)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Iteration   1 - Elapsed 0.17s - training error: 2.27769995
Iteration   2 - Elapsed 0.34s - training error: 2.19313025
Iteration   3 - Elapsed 0.51s - training error: 2.19323778
Iteration   4 - Elapsed 0.66s - training error: 2.19308257
Iteration   5 - Elapsed 0.82s - training error: 2.19269753
Iteration   6 - Elapsed 0.98s - training error: 2.19266987
Iteration   7 - Elapsed 1.13s - training error: 2.19262886
Iteration   8 - Elapsed 1.29s - training error: 2.19262505
Iteration   9 - Elapsed 1.45s - training error: 2.19262052
Iteration  10 - Elapsed 1.76s - training error: 2.19260979
Iteration  11 - Elapsed 1.92s - training error: 2.19261813
Iteration  12 - Elapsed 2.08s - training error: 2.19261646
Iteration  13 - Elapsed 2.25s - training error: 2.19263911
Iteration  14 - Elapsed 2.42s - training error: 2.19263911
Iteration  15 - Elapsed 2.58s - training error: 2.19264960
Iteration  16 - Elapsed 2.74s - training error: 2.19265103
Iteration  17 - Elapsed 2.91s - training error: 2.19268680
Iteration  18 - Elapsed 3.07s - training error: 2.19269395
Iteration  19 - Elapsed 3.23s - training error: 2.19270301
Iteration  20 - Elapsed 3.55s - training error: 2.19275403
Test RMSE: 2.19
</pre></div></div>
</div>
</section>
</section>
<section id="Supporting-Sparse-Data">
<h2>Supporting Sparse Data<a class="headerlink" href="#Supporting-Sparse-Data" title="Permalink to this heading"></a></h2>
<p>Sparse support can be necessary for kernel learning in extremely high dimensions, when the inputs are sparse.</p>
<p>Sparse support requires using special functions for common operations such as matrix multiplication. Falkon implements sparse tensors in a CSR format (PyTorch is slowly picking this format up, in place of COO), through the <code class="docutils literal notranslate"><span class="pre">falkon.sparse.SparseTensor</span></code> class.</p>
<p>We will implement the <code class="docutils literal notranslate"><span class="pre">compute_sparse</span></code> method below, supporting both diagonal and full kernels. However, only CPU support is added here (CUDA support is possible but requires a few more details), and differentiable sparse kernels are not supported.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from falkon.sparse import SparseTensor
from falkon.sparse import sparse_matmul, bdot
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class SparseLinearKernel(Kernel):
    def __init__(self, lengthscale, options):
        # The base class takes as inputs a name for the kernel, and
        # an instance of `FalkonOptions`.
        super().__init__(&quot;sparse_linear&quot;, options)

        self.lengthscale = lengthscale

    def compute(self, X1: torch.Tensor, X2: torch.Tensor, out: torch.Tensor, diag: bool) -&gt; torch.Tensor:
        lengthscale = self.lengthscale.to(device=X1.device, dtype=X1.dtype)

        scaled_X1 = X1 * lengthscale

        if diag:
            out.copy_(torch.sum(scaled_X1 * X2, dim=-1))
        else:
            # The dot-product row-by-row on `X1` and `X2` can be computed
            # on many rows at a time with matrix multiplication.
            out = torch.matmul(scaled_X1, X2.T, out=out)

        return out

    def compute_sparse(self,
                       X1: SparseTensor,
                       X2: SparseTensor,
                       out: torch.Tensor,
                       diag: bool,
                       **kwargs) -&gt; torch.Tensor:
        # The inputs will be matrix X1(n*d) in CSR format, and X2(d*n) in CSC format.

        # To support different devices/data types, you must make sure
        # the lengthscale is compatible with the data.
        lengthscale = self.lengthscale.to(device=X1.device, dtype=X1.dtype)

        if diag:
            # The diagonal is a dot-product between rows of X1 and X2.
            # The batched-dot is only implemented on CPU.
            out = bdot(X1, X2.transpose_csr(), out)
        else:
            # Otherwise we need to matrix-multiply. Note that X2 is already
            # transposed correctly.
            out = sparse_matmul(X1, X2, out)

        out.mul_(lengthscale)
        return out
</pre></div>
</div>
</div>
<section id="Testing-sparse-support">
<h3>Testing sparse support<a class="headerlink" href="#Testing-sparse-support" title="Permalink to this heading"></a></h3>
<p>We generate two sparse matrices, and check that the sparse kernel is equivalent to the dense version.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>indexptr = torch.tensor([0, 1, 3, 4], dtype=torch.long)
index = torch.tensor([1, 0, 1, 0], dtype=torch.long)
value = torch.tensor([5, 1, 8, 2], dtype=torch.float32)
sp1 = SparseTensor(indexptr=indexptr, index=index, data=value, size=(3, 2), sparse_type=&quot;csr&quot;)
# Converted to dense:
dense1 = torch.from_numpy(sp1.to_scipy().todense())
dense1
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor([[0., 5.],
        [1., 8.],
        [2., 0.]])
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>indexptr = torch.tensor([0, 1, 2, 4], dtype=torch.long)
index = torch.tensor([1, 0, 0, 1], dtype=torch.long)
value = torch.tensor([2, 1, 3, 4], dtype=torch.float32)
sp2 = SparseTensor(indexptr=indexptr, index=index, data=value, size=(3, 2), sparse_type=&quot;csr&quot;)
dense2 = torch.from_numpy(sp2.to_scipy().todense())
dense2
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor([[0., 2.],
        [1., 0.],
        [3., 4.]])
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Initialize the kernel
lengthscale_init = torch.tensor([1.0])
k = SparseLinearKernel(lengthscale_init, options=falkon.FalkonOptions())
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>k(sp1, sp2) == k(dense1, dense2)
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor([[True, True, True],
        [True, True, True],
        [True, True, True]])
</pre></div></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="falkon_cv.html" class="btn btn-neutral float-left" title="Hyperparameter Tuning with Falkon" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="hyperopt.html" class="btn btn-neutral float-right" title="Automatic Hyperparameter Optimization" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Giacomo Meanti, Alessandro Rudi.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>